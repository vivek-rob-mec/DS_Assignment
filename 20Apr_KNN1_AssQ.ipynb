{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is the KNN algorithm?\n",
        "Ans: \\\n",
        "**KNN (K-Nearest Neighbors)** is a simple, **instance-based** machine learning algorithm used for **classification** and **regression**.\n",
        "\n",
        "---\n",
        "\n",
        "###  Key Idea:\n",
        "> A data point is classified based on how its **nearest neighbors** are labeled.\n",
        "\n",
        "---\n",
        "\n",
        "###  How It Works (for classification):\n",
        "\n",
        "1. Choose the number of neighbors **K**.\n",
        "2. Measure distance (e.g., Euclidean) between the new point and all training points.\n",
        "3. Find the **K closest points**.\n",
        "4. Assign the **most common class** among those neighbors.\n",
        "\n",
        "---\n",
        "\n",
        "###  For Regression:\n",
        "- Predict the **average** of the K nearest neighbors’ values instead of voting for class.\n",
        "\n",
        "---\n",
        "\n",
        "###  Key Parameters:\n",
        "- **K**: Number of neighbors (small K = sensitive to noise; large K = smoother)\n",
        "- **Distance metric**: Euclidean, Manhattan, etc.\n",
        "\n",
        "---\n",
        "\n",
        "###  Pros:\n",
        "- Simple to understand and use\n",
        "- No training time\n",
        "\n",
        "###  Cons:\n",
        "- Slow with large datasets\n",
        "- Affected by irrelevant or scaled features\n",
        "\n",
        "---\n",
        "\n",
        "###  In Short:\n",
        "> **KNN** assigns labels based on the **closest K neighbors** — it's like \"asking your neighbors what they would do\" and following the majority!"
      ],
      "metadata": {
        "id": "w2svZ5nW7rq6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. How do you choose the value of K in KNN?\n",
        "Ans: \\\n",
        "\n",
        "Choosing the right **K** is crucial for the performance of the **K-Nearest Neighbors** algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "###  Tips for Choosing K:\n",
        "\n",
        "1. **Try odd values** (like 1, 3, 5...)  \n",
        "   → Prevents ties in **binary classification**.\n",
        "\n",
        "2. **Use Cross-Validation**  \n",
        "   → Try different K values and pick the one with the **lowest validation error**.\n",
        "\n",
        "3. **Start with K ≈ √N**  \n",
        "   → A common rule of thumb, where **N** = number of training samples.\n",
        "\n",
        "---\n",
        "\n",
        "###  Effect of K:\n",
        "\n",
        "| K Value   | Behavior                         | Risk                              |\n",
        "|-----------|----------------------------------|-----------------------------------|\n",
        "| **Small K (e.g., 1)** | Very sensitive to noise             | Overfitting                       |\n",
        "| **Large K**           | Smoother decision boundary          | Underfitting (too generalized)    |\n",
        "\n",
        "---\n",
        "\n",
        "###  In Short:\n",
        "- **Too small K** = overfitting  \n",
        "- **Too large K** = underfitting  \n",
        "- **Best K** = found using **cross-validation** for balanced performance."
      ],
      "metadata": {
        "id": "lo9YNYvN7r4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. What is the difference between KNN classifier and KNN regressor?\n",
        "Ans: \\\n",
        "\n",
        "| Feature           | **KNN Classifier**                         | **KNN Regressor**                        |\n",
        "|-------------------|--------------------------------------------|------------------------------------------|\n",
        "| **Goal**          | Predict **class/label**                    | Predict **continuous value**             |\n",
        "| **Output**        | Most frequent class among K neighbors      | Average (or weighted average) of neighbors’ values |\n",
        "| **Use Case**      | Spam detection, disease diagnosis, etc.    | Predicting house prices, temperatures, etc. |\n",
        "| **Decision Logic**| **Majority voting**                        | **Mean or median** of K values           |\n",
        "| **Evaluation**    | Accuracy, Precision, Recall, F1-score      | RMSE, MAE, R²                             |\n",
        "\n",
        "---\n",
        "\n",
        "###  Example:\n",
        "\n",
        "- **Classifier**:  \n",
        "  Predict whether an email is spam or not by checking how neighboring emails are labeled.\n",
        "\n",
        "- **Regressor**:  \n",
        "  Predict the **price of a house** based on nearby houses' prices.\n",
        "\n",
        "---\n",
        "\n",
        "###  In Short:\n",
        "- **KNN Classifier** → majority **label** vote.  \n",
        "- **KNN Regressor** → average **numeric** value.  \n",
        "Same algorithm, different target types."
      ],
      "metadata": {
        "id": "8WlNr7Wm7r9i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. How do you measure the performance of KNN?\n",
        "Ans: \\\n",
        "It depends on whether you're using **KNN for classification** or **regression**.\n",
        "\n",
        "---\n",
        "\n",
        "###  For **KNN Classifier**:\n",
        "\n",
        "| Metric        | Description                            |\n",
        "|---------------|----------------------------------------|\n",
        "| **Accuracy**  | % of correct predictions                |\n",
        "| **Precision** | Correct positives out of predicted positives |\n",
        "| **Recall**    | Correct positives out of actual positives |\n",
        "| **F1-score**  | Harmonic mean of precision and recall  |\n",
        "| **Confusion Matrix** | Shows TP, TN, FP, FN              |\n",
        "\n",
        " **Use when**: Evaluating classification tasks like spam detection or disease diagnosis.\n",
        "\n",
        "---\n",
        "\n",
        "###  For **KNN Regressor**:\n",
        "\n",
        "| Metric       | Description                             |\n",
        "|--------------|-----------------------------------------|\n",
        "| **MAE**      | Mean Absolute Error                     |\n",
        "| **MSE / RMSE** | Mean (or Root Mean) Squared Error     |\n",
        "| **R² Score** | How well the model explains the variance |\n",
        "\n",
        " **Use when**: Predicting numerical values like prices or temperatures.\n",
        "\n",
        "---\n",
        "\n",
        "###  In Short:\n",
        "- **Classifier** → Accuracy, F1-score, etc.  \n",
        "- **Regressor** → MAE, RMSE, R²  \n",
        "Use **cross-validation** to get reliable results!"
      ],
      "metadata": {
        "id": "QGaMnpZ87sA4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. What is the curse of dimensionality in KNN?\n",
        "Ans: \\\n",
        "The **curse of dimensionality** refers to problems that arise when working with **high-dimensional data** — especially for distance-based algorithms like **KNN**.\n",
        "\n",
        "---\n",
        "\n",
        "###  Key Idea:\n",
        "> As the number of features (**dimensions**) increases, **data points become increasingly sparse**, and **distance measures lose meaning**.\n",
        "\n",
        "---\n",
        "\n",
        "###  Why It’s a Problem for KNN:\n",
        "\n",
        "- KNN relies on **distance (e.g., Euclidean)** to find “nearest” neighbors.\n",
        "- In **high dimensions**, all points tend to become **equally far apart**.\n",
        "- This makes it **hard for KNN to find meaningful neighbors** → performance drops.\n",
        "\n",
        "---\n",
        "\n",
        "###  Example:\n",
        "- In 2D, it's easy to see which points are close.\n",
        "- In 100D, even the nearest neighbor might be **far** in terms of distance.\n",
        "\n",
        "---\n",
        "\n",
        "###  Effects:\n",
        "- Poor accuracy  \n",
        "- Increased computation  \n",
        "- Overfitting or underfitting\n",
        "\n",
        "---\n",
        "\n",
        "###  Solutions:\n",
        "- **Feature selection**  \n",
        "- **Dimensionality reduction** (e.g., PCA, t-SNE)  \n",
        "- **Scaling** the data properly\n",
        "\n",
        "---\n",
        "\n",
        "###  In Short:\n",
        "> The **curse of dimensionality** makes KNN less effective as dimensions increase, because **distance stops being meaningful**."
      ],
      "metadata": {
        "id": "bt4UlKWe7sEa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6. How do you handle missing values in KNN?\n",
        "Ans: \\\n",
        "Handling missing values is important because **KNN is distance-based**, and missing data can **distort the distance** calculations.\n",
        "\n",
        "---\n",
        "\n",
        "###  Common Ways to Handle Missing Values in KNN:\n",
        "\n",
        "#### 1. **Impute Before Applying KNN**\n",
        "- **Mean/Median/Mode Imputation**  \n",
        "  → Fill missing values with the mean (numerical), median, or mode (categorical).\n",
        "  \n",
        "- **KNN Imputation**  \n",
        "  → Use **K-nearest complete rows** to estimate missing values based on similarity.\n",
        "\n",
        "```python\n",
        "from sklearn.impute import KNNImputer\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "X_filled = imputer.fit_transform(X)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Drop Rows or Features (if minimal missing data)**\n",
        "- Remove rows/columns with missing values if the impact is small.\n",
        "\n",
        "---\n",
        "\n",
        "###  Avoid:\n",
        "- Leaving missing values unprocessed — KNN can't handle them directly.\n",
        "- Using arbitrary constants (like 9999) unless domain-justified.\n",
        "\n",
        "---\n",
        "\n",
        "###  In Short:\n",
        "> Fill in missing values **before using KNN**, ideally using **KNN Imputation** for more accurate results."
      ],
      "metadata": {
        "id": "IBouWAkL7sH2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n",
        "Ans: \\\n",
        "\n",
        "| Aspect                | **KNN Classifier**                          | **KNN Regressor**                           |\n",
        "|------------------------|---------------------------------------------|---------------------------------------------|\n",
        "| **Output**            | Class label                                 | Continuous value                            |\n",
        "| **Decision Rule**     | Majority vote of neighbors                  | Average (or weighted average) of neighbors  |\n",
        "| **Evaluation Metrics**| Accuracy, F1-score, etc.                    | MAE, RMSE, R²                                |\n",
        "| **Use Cases**         | Spam detection, image recognition, diagnosis| House price, temperature, stock prediction  |\n",
        "| **Sensitivity to Outliers** | Less sensitive (depends on K)          | More sensitive (outliers skew average)      |\n",
        "| **Handling Noise**    | Can be affected by mislabeled data          | Can be smoothed with larger K               |\n",
        "\n",
        "---\n",
        "\n",
        "###  Which One is Better?\n",
        "\n",
        "- **Use KNN Classifier** for:  \n",
        "  → Problems with **discrete categories** (e.g., cat vs. dog, spam vs. ham)\n",
        "\n",
        "- **Use KNN Regressor** for:  \n",
        "  → Problems needing **numeric predictions** (e.g., price, temperature)\n",
        "\n",
        "---\n",
        "\n",
        "###  In Short:\n",
        "> KNN **classifier** is best for **label prediction**,  \n",
        "> KNN **regressor** is best for **value prediction** — both rely on neighbor similarity but are used for **different types of output**."
      ],
      "metadata": {
        "id": "a1nM64Oj7sLI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n",
        "Ans: \\\n",
        "\n",
        "###  **Strengths of KNN**:\n",
        "\n",
        "1. **Simple & Intuitive**  \n",
        "   - Easy to understand and implement.\n",
        "\n",
        "2. **No Training Phase**  \n",
        "   - It's a **lazy learner** → just stores data.\n",
        "\n",
        "3. **Works Well with Well-Separated Data**  \n",
        "   - Performs well when similar points are close together.\n",
        "\n",
        "4. **Flexible for Classification & Regression**  \n",
        "   - Can be used for both types of tasks.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Weaknesses of KNN**:\n",
        "\n",
        "| Weakness                  | Description                                           | Fix/Workaround                          |\n",
        "|---------------------------|-------------------------------------------------------|------------------------------------------|\n",
        "| **Slow at Prediction**    | Must compute distance to all points at runtime       | Use **KD-tree**, **Ball tree**, or **approx. nearest neighbors** |\n",
        "| **Sensitive to Irrelevant Features** | Unimportant features distort distances        | Use **feature selection** or **PCA**     |\n",
        "| **Affected by Scale**     | Larger-scale features dominate distance              | Apply **normalization/standardization**  |\n",
        "| **Struggles with High Dimensions** | Distance loses meaning in high-D space         | Use **dimensionality reduction**         |\n",
        "| **Memory Intensive**      | Needs to store the whole dataset                    | Use sampling or compress dataset         |\n",
        "| **Sensitive to Noisy Data / Outliers** | One noisy neighbor can mislead results      | Increase **K** or use **weighted KNN**   |\n",
        "\n",
        "---\n",
        "\n",
        "###  In Short:\n",
        "\n",
        "**KNN is:**\n",
        "-  Great for simple, small-scale problems  \n",
        "-  Challenged by big, high-dimensional, or noisy datasets  \n",
        "But with the right **preprocessing and optimizations**, it can still perform well!"
      ],
      "metadata": {
        "id": "l_XZxpbW7sOc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n",
        "Ans: \\\n",
        "\n",
        "###  **Euclidean Distance** (L2 norm)\n",
        "\n",
        "- Measures the **straight-line** distance between two points.  \n",
        "- Formula (2D):  \n",
        "  $$[\n",
        "  \\text{Euclidean}(A, B) = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}\n",
        "  ]$$\n",
        "\n",
        "- **Visual**: Like using a ruler to measure the shortest path.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Manhattan Distance** (L1 norm)\n",
        "\n",
        "- Measures the **grid-like path** (like moving in a city with blocks).  \n",
        "- Formula (2D):  \n",
        "  $$[\n",
        "  \\text{Manhattan}(A, B) = |x_1 - x_2| + |y_1 - y_2|\n",
        "  ]$$\n",
        "\n",
        "- **Visual**: Like walking along streets in a city — no diagonal moves.\n",
        "\n",
        "---\n",
        "\n",
        "###  Key Differences:\n",
        "\n",
        "| Feature              | Euclidean Distance                     | Manhattan Distance                      |\n",
        "|----------------------|----------------------------------------|-----------------------------------------|\n",
        "| Path Type            | Straight-line (diagonal allowed)       | Block-by-block (no diagonals)           |\n",
        "| Sensitive to Scale   | More sensitive                         | Less sensitive                          |\n",
        "| Best for             | Dense, spherical clusters              | Grid-based or high-dimensional data     |\n",
        "| Formula Type         | Uses squares and square roots          | Uses absolute differences                |\n",
        "\n",
        "---\n",
        "\n",
        "###  Which One to Use in KNN?\n",
        "\n",
        "- **Euclidean**: When distance in space matters (e.g., image data).  \n",
        "- **Manhattan**: When working with **high dimensions** or grid-like data.\n",
        "\n",
        "---\n",
        "\n",
        "###  In Short:\n",
        "> **Euclidean** = \"as the crow flies\"  \n",
        "> **Manhattan** = \"city block distance\"  "
      ],
      "metadata": {
        "id": "oQt5p-6f7sR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4tbz_gVi7sVo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q10. What is the role of feature scaling in KNN?\n",
        "Ans: \\\n",
        "\n",
        "###  Key Idea:\n",
        "> **KNN is a distance-based algorithm**, so features with **larger ranges** can **dominate** the distance calculation — even if they aren’t more important.\n",
        "\n",
        "---\n",
        "\n",
        "###  Why Scaling Matters:\n",
        "\n",
        "- KNN uses distances (like **Euclidean** or **Manhattan**)  \n",
        "- Without scaling, a feature like “salary (in ₹)” can overshadow “age” or “rating”\n",
        "\n",
        "---\n",
        "\n",
        "###  Common Scaling Methods:\n",
        "\n",
        "| Method        | Description                                 |\n",
        "|---------------|---------------------------------------------|\n",
        "| **Standardization** | Convert to zero mean and unit variance (Z-score) |\n",
        "| **Min-Max Scaling** | Scale values to a fixed range (usually 0 to 1)   |\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  Benefits of Scaling:\n",
        "\n",
        "- Fair distance calculation  \n",
        "- Improved model performance  \n",
        "- Faster convergence (if used with optimization-based models)\n",
        "\n",
        "---\n",
        "\n",
        "###  In Short:\n",
        "> Feature scaling **ensures all features contribute equally** to the distance in KNN — it's **essential** for good performance."
      ],
      "metadata": {
        "id": "6VAKMbfb7sZO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wKqIx2xeAUvT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}