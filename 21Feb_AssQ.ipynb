{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "ztlM-2h2epu-",
        "outputId": "64673961-36d1-4b85-dfd3-001937f3aac0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nWeb scraping is the process of collecting unstructured and structured data in an automated manner. \\nIt’s also widely known as web data extraction or web data scraping.\\n\\nA web scraper automates the process of extracting information from other websites, quickly and accurately.\\n\\n1) The process is extremely simple and works by way of two parts: a web crawler and a web scraper.\\n2) The web crawler is the horse, and the scraper is the chariot.\\n3) The crawler leads the scraper, as if by hand, through the internet, where it extracts the data requested.\\n4) A web crawler, which we generally call a “spider,” is an artificial intelligence that browses the internet \\n   to index and search for content by following links and exploring\\n5) In many projects, you first “crawl” the web or one specific website to discover URLs which then you pass on to your scraper.\\n6) A web scraper is a specialized tool designed to accurately and quickly extract data from a web page.\\n\\nWeb scraping process looks like:\\n\\n1) Identify the target website\\n2) Collect URLs of the target pages\\n3) Make a request to these URLs to get the HTML of the page\\n4) Use locators to find the information in the HTML\\n5)Save the data in a JSON or CSV file or some other structured format\\n\\nUsed for:\\n1) real-estate\\n2) market research\\n3) News and content monitoring\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data.\n",
        "\"\"\"\n",
        "Web scraping is the process of collecting unstructured and structured data in an automated manner.\n",
        "It’s also widely known as web data extraction or web data scraping.\n",
        "\n",
        "A web scraper automates the process of extracting information from other websites, quickly and accurately.\n",
        "\n",
        "Types of Web Scrapers:\n",
        "Browser Extension Scrapers: Scrapers that work as browser extensions, useful for scraping small bits of data from specific web pages.\n",
        "Software-Based Scrapers: Software that can be installed locally on your computer.\n",
        "Cloud-Based Scrapers: Scrapers that run in the cloud, offering scalability and ease of use.\n",
        "\n",
        "1) The process is extremely simple and works by way of two parts: a web crawler and a web scraper.\n",
        "2) The web crawler is the horse, and the scraper is the chariot.\n",
        "3) The crawler leads the scraper, as if by hand, through the internet, where it extracts the data requested.\n",
        "4) A web crawler, which we generally call a “spider,” is an artificial intelligence that browses the internet\n",
        "   to index and search for content by following links and exploring\n",
        "5) In many projects, you first “crawl” the web or one specific website to discover URLs which then you pass on to your scraper.\n",
        "6) A web scraper is a specialized tool designed to accurately and quickly extract data from a web page.\n",
        "\n",
        "Web scraping process looks like:\n",
        "\n",
        "1) Identify the target website\n",
        "2) Collect URLs of the target pages\n",
        "3) Make a request to these URLs to get the HTML of the page\n",
        "4) Use locators to find the information in the HTML\n",
        "5)Save the data in a JSON or CSV file or some other structured format\n",
        "\n",
        "Used for:\n",
        "1) real-estate\n",
        "2) market research\n",
        "3) News and content monitoring\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2. What are the different methods used for Web Scraping?\n",
        "\"\"\"\n",
        "1) HTML Parsing: This involves using tools to analyze the structure of a web page's HTML code to extract specific\n",
        "                 data elements like text, images, links, and tables.\n",
        "2) DOM Parsing: Similar to HTML parsing, but uses the Document Object Model (DOM) to view the structure of web pages and extract data.\n",
        "3) Regular Expressions (Regex): Using patterns to search and extract data from text or HTML.\n",
        "4) Manual Scraping: Copying and pasting data from websites, or using screen capturing tools.\n",
        "5) XPath: A language for navigating through tree-like structures in HTML or XML documents to extract data\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "i3OjPfRfhydE",
        "outputId": "2e3e70b6-6bc4-4375-e087-b252f172b5d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n1) HTML Parsing: This involves using tools to analyze the structure of a web page's HTML code to extract specific \\n                 data elements like text, images, links, and tables. \\n2) DOM Parsing: Similar to HTML parsing, but uses the Document Object Model (DOM) to view the structure of web pages and extract data. \\n3) Regular Expressions (Regex): Using patterns to search and extract data from text or HTML. \\n4) Manual Scraping: Copying and pasting data from websites, or using screen capturing tools. \\n5) XPath: A language for navigating through tree-like structures in HTML or XML documents to extract data\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3. What is Beautiful Soup? Why is it used?\n",
        "\"\"\"\n",
        "1) Beautiful Soup is a valuable tool for web scraping, and it seamlessly integrates with several different XML and HTML parsers.\n",
        "Once you identify the data you want to scrape and understand the web page’s structure,\n",
        "you can use the Beautiful Soup Python library to write a script quickly.\n",
        "\n",
        "2) The library automatically selects the best HTML parser on your device, or you can specify a custom HTML parser.\n",
        "Then the library converts the HTML document into a navigable tree of Python objects.\n",
        "\n",
        "3) The Beautiful Soup HTML or XML parser can be used to parse web content and generate Python objects that resemble a DOM tree.\n",
        "The generated Python object can be used to extract data from different parts of the document efficiently by selecting the relevant elements\n",
        "\n",
        "4) There are several approaches for selecting elements, including find(), which takes a selector condition and returns the first matching HTML\n",
        "element, and find_all(), which takes a selector condition and returns a list of all matching HTML elements\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "hcAtqJ_Ekxsv",
        "outputId": "99ba311f-7b7c-4789-cdde-429100e4315a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n1) Beautiful Soup is a valuable tool for web scraping, and it seamlessly integrates with several different XML and HTML parsers. \\nOnce you identify the data you want to scrape and understand the web page’s structure, \\nyou can use the Beautiful Soup Python library to write a script quickly.\\n\\n2) The library automatically selects the best HTML parser on your device, or you can specify a custom HTML parser. \\nThen the library converts the HTML document into a navigable tree of Python objects.\\n\\n3) The Beautiful Soup HTML or XML parser can be used to parse web content and generate Python objects that resemble a DOM tree. \\nThe generated Python object can be used to extract data from different parts of the document efficiently by selecting the relevant elements\\n\\n4) There are several approaches for selecting elements, including find(), which takes a selector condition and returns the first matching HTML \\nelement, and find_all(), which takes a selector condition and returns a list of all matching HTML elements\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4. Why is flask used in this Web Scraping project?\n",
        "\"\"\"\n",
        "Flask is an excellent choice for web scraping projects: Simplicity and Flexibility: Flask's minimalistic design allows\n",
        "developers to set up and customize their web scraping applications quickly\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "WBL9jugLmXRY",
        "outputId": "a5308289-7db2-4b17-ab18-29d964816c2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nFlask is an excellent choice for web scraping projects: Simplicity and Flexibility: Flask's minimalistic design allows\\ndevelopers to set up and customize their web scraping applications quickly\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5. Write the names of AWS services used in this project. Also, explain the use of each service.\n",
        "\"\"\"\n",
        "IAM : To manage access to AWS resources\n",
        "Elastic Beanstalk : To manage and run Web apps\n",
        "Code Pipeline : Release software using continous delivery\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BYK4Sm9cnCKK",
        "outputId": "e36308c7-1737-409a-eb0c-159a2e578888"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nIAM : To manage access to AWS resources\\nElastic Beanstalk : To manage and run Web apps\\nCode Pipeline : Release software using continous delivery\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ptNvyE0dz9TF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}