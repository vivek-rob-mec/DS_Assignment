{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is the Filter method in feature selection, and how does it work?\n",
        "Ans: \\\n",
        "\n",
        "###  **Definition:**\n",
        "\n",
        "The **Filter method** is a **feature selection technique** used in machine learning to select important features **independently of any machine learning model**. It evaluates the relevance of each feature based on **statistical measures** and ranks them accordingly.\n",
        "\n",
        "---\n",
        "\n",
        "###  **How It Works:**\n",
        "\n",
        "1. **Calculate a score** (relevance) for each feature using a statistical test or metric.\n",
        "2. **Rank the features** based on their scores.\n",
        "3. **Select the top-k features** or those above a certain threshold.\n",
        "4. Discard the rest.\n",
        "\n",
        "This process is done **before** training any model — it’s completely **model-agnostic**.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Common Statistical Measures Used:**\n",
        "\n",
        "- **Correlation Coefficient:** Measures how strongly a feature is linearly related to the target variable (for regression).\n",
        "- **Chi-Square Test:** Used for categorical features and target (classification).\n",
        "- **ANOVA F-test:** Compares means between groups (for classification tasks).\n",
        "- **Mutual Information:** Measures the amount of shared information between feature and target.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Advantages:**\n",
        "\n",
        "- Very **fast and scalable**\n",
        "- Works well with **high-dimensional datasets**\n",
        "- Helps reduce **overfitting**\n",
        "- Simplifies the model by removing irrelevant or redundant features\n",
        "\n",
        "---\n",
        "\n",
        "###  **Limitations:**\n",
        "\n",
        "- Ignores **feature interactions**  \n",
        "- May select features that look good individually but **don’t perform well together**\n",
        "\n",
        "---\n",
        "\n",
        "###  **Example Use Case:**\n",
        "\n",
        "Suppose you're predicting whether a person has a disease (yes/no) based on 50 features. Using the Filter method, you might:\n",
        "- Compute the **correlation** or **chi-square** value for each feature with the target.\n",
        "- Keep only the **top 10 most relevant features**.\n",
        "- Train your model on those"
      ],
      "metadata": {
        "id": "IQdm9_YhrtK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. How does the Wrapper method differ from the Filter method in feature selection?\n",
        "Ans: \\\n",
        "\n",
        "###  **Overview:**\n",
        "\n",
        "Both **Filter** and **Wrapper** methods are used to **select relevant features** from a dataset, but they differ in **how** they evaluate those features.\n",
        "\n",
        "---\n",
        "\n",
        "###  **1. Filter Method:**\n",
        "\n",
        "- **Model-independent**  \n",
        "- Uses **statistical techniques** (like correlation, chi-square, mutual information)  \n",
        "- Evaluates each feature **individually**  \n",
        "- **Fast** and **scalable**, especially for large datasets\n",
        "\n",
        " *Example:* Selecting top features based on Pearson correlation with the target.\n",
        "\n",
        "---\n",
        "\n",
        "###  **2. Wrapper Method:**\n",
        "\n",
        "- **Model-dependent**  \n",
        "- Uses a **machine learning algorithm** to evaluate feature subsets  \n",
        "- Searches for the **best combination** of features by actually training the model multiple times  \n",
        "- **Slower** and **computationally expensive**, but often **more accurate**\n",
        "\n",
        " *Example:* Using Recursive Feature Elimination (RFE) with a decision tree to find the best set of features.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Key Differences:**\n",
        "\n",
        "| Aspect            | Filter Method                         | Wrapper Method                          |\n",
        "|-------------------|----------------------------------------|------------------------------------------|\n",
        "| Model Use         | Does **not** use ML model              | **Uses** ML model for evaluation         |\n",
        "| Speed             | **Fast** (only stats-based)            | **Slow** (trains multiple models)        |\n",
        "| Accuracy          | Less accurate than wrapper             | More accurate, considers feature interaction |\n",
        "| Feature Evaluation| One-by-one or individually             | Evaluates **combinations** of features   |\n",
        "| Scalability       | Scales well to high dimensions         | Less scalable to large datasets          |\n",
        "| Overfitting Risk  | Lower                                  | Higher (due to model overuse)            |\n",
        "\n",
        "---\n",
        "\n",
        "###  **In Summary:**\n",
        "\n",
        "- **Filter method** is like a quick pre-check using statistics.\n",
        "- **Wrapper method** is like trying different feature sets in a real model and picking the best-performing combination."
      ],
      "metadata": {
        "id": "9LIDq5CkrtPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. What are some common techniques used in Embedded feature selection methods?\n",
        "Ans: \\\n",
        "\n",
        "###  **What is Embedded Feature Selection?**\n",
        "\n",
        "**Embedded methods** combine the advantages of both **filter** and **wrapper** methods. Feature selection happens **during the model training process** — meaning the model itself decides which features are important while it's learning.\n",
        "\n",
        "> It’s model-based and more efficient than wrapper methods because it selects features as part of the training.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Common Techniques in Embedded Methods:**\n",
        "\n",
        "---\n",
        "\n",
        "#### **1. Lasso Regression (L1 Regularization)**\n",
        "- Adds a penalty equal to the absolute value of the coefficients.\n",
        "- Shrinks some coefficients to **exactly zero**, effectively removing less important features.\n",
        "- Great for **sparse** models and **automatic feature elimination**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Ridge Regression (L2 Regularization)**  \n",
        "- Penalizes large coefficients (but doesn’t shrink them to zero).\n",
        "- Helps in **reducing model complexity**, but doesn’t perform strict feature selection.\n",
        "\n",
        " *Note: Ridge helps control overfitting but not feature selection directly.*\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Elastic Net**\n",
        "- Combines **L1 and L2** penalties.\n",
        "- Can both **select features** and **handle multicollinearity**.\n",
        "- Useful when you have **many correlated features**.\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. Decision Tree-Based Models**\n",
        "- Models like **Decision Trees, Random Forests, Gradient Boosted Trees** (e.g., XGBoost, LightGBM) provide **feature importance scores**.\n",
        "- These scores can be used to select the most relevant features.\n",
        "- Embedded because feature importance is evaluated as the model is being trained.\n",
        "\n",
        "---\n",
        "\n",
        "#### **5. Recursive Feature Elimination with Built-in Models (e.g., RFE with SVM/Logistic Regression)**\n",
        "- Although often considered a wrapper technique, when combined with **regularized models**, it can act as an embedded method.\n",
        "- The model recursively removes least important features based on coefficients or weights.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Summary:**\n",
        "\n",
        "| Technique                | Model Type        | Feature Selection Mechanism            |\n",
        "|--------------------------|-------------------|-----------------------------------------|\n",
        "| Lasso (L1)               | Linear models      | Shrinks coefficients to 0               |\n",
        "| Elastic Net              | Linear models      | Mix of L1 and L2                        |\n",
        "| Tree-based models        | Non-linear models  | Use built-in feature importance         |\n",
        "| RFE + regularized models | Hybrid             | Recursive elimination using model scores"
      ],
      "metadata": {
        "id": "D79n-LOLrtT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. What are some drawbacks of using the Filter method for feature selection?\n",
        "Ans: \\\n",
        "\n",
        "While the **Filter method** is fast and easy to use, it comes with some important **limitations** that can affect the performance of your machine learning model.\n",
        "\n",
        "---\n",
        "\n",
        "###  **1. Ignores Feature Interactions**\n",
        "\n",
        "- It evaluates each feature **independently** of others.\n",
        "- Doesn’t consider **combinations** or **dependencies** between features.\n",
        "  \n",
        " *Example:* Two features might be weak alone but powerful when used together — filter methods won't detect that.\n",
        "\n",
        "---\n",
        "\n",
        "###  **2. Not Model-Specific**\n",
        "\n",
        "- Filter methods are **model-agnostic**, meaning they don’t consider the learning algorithm.\n",
        "- A feature might seem statistically relevant but be **useless for a specific model** (like decision trees or SVMs).\n",
        "\n",
        "---\n",
        "\n",
        "###  **3. May Select Redundant Features**\n",
        "\n",
        "- Since it doesn’t consider correlation between features, it might keep **multiple features that carry the same information**.\n",
        "- This can lead to **unnecessary complexity** and **multicollinearity**.\n",
        "\n",
        "---\n",
        "\n",
        "###  **4. Doesn’t Optimize for Accuracy**\n",
        "\n",
        "- Selection is based on statistical scores, **not on model performance**.\n",
        "- As a result, you might end up with features that look good statistically but don't improve (or even hurt) predictive accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "###  **5. Not Robust to Noisy Data**\n",
        "\n",
        "- Filter methods can be sensitive to noise.\n",
        "- Noisy or irrelevant features may still show strong statistical correlation and get wrongly selected.\n",
        "\n",
        "---\n",
        "\n",
        "###  **When to Use Filter Methods:**\n",
        "\n",
        "- You have a **very high-dimensional dataset** (e.g., genomics, text data).\n",
        "- You need a **quick pre-processing step** before applying more advanced methods.\n",
        "- As a **first step** before wrapper or embedded methods"
      ],
      "metadata": {
        "id": "bR41Vxw7rtY4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. In which situations would you prefer using the Filter method over the Wrapper method for feature selection?\n",
        "Ans: \\\n",
        "The **Filter method** is often the better choice when **speed, simplicity, and scalability** are more important than the absolute best model performance.\n",
        "\n",
        "Here are situations where you'd prefer **Filter over Wrapper**:\n",
        "\n",
        "---\n",
        "\n",
        "###  **1. When Working with High-Dimensional Data**\n",
        "\n",
        "- In datasets with **hundreds or thousands of features** (like genomics, text data), wrapper methods become **too slow and computationally expensive**.\n",
        "- Filter methods are **much faster** and help reduce dimensionality before deeper analysis.\n",
        "\n",
        "---\n",
        "###  **2. As a Preprocessing Step**\n",
        "\n",
        "- You can use the filter method to **remove obviously irrelevant features** before applying wrapper or embedded methods.\n",
        "- This improves **training time** and makes the model easier to tune.\n",
        "\n",
        "---\n",
        "\n",
        "###  **3. When You Need a Quick Baseline**\n",
        "\n",
        "- For **initial experiments** or exploratory analysis, filter methods are perfect to get a quick idea of which features might matter.\n",
        "- Helps in building a **prototype** fast.\n",
        "\n",
        "---\n",
        "\n",
        "###  **4. When Model Interpretability Is Not a Priority**\n",
        "\n",
        "- Since filter methods are simple and based on basic statistics, they’re suitable when you're just narrowing down the feature set without needing model-specific insights.\n",
        "\n",
        "---\n",
        "\n",
        "###  **5. When You Want Model Independence**\n",
        "\n",
        "- Filter methods are **not tied to any machine learning model**, so you can use the same selected features across different models (e.g., try both logistic regression and random forest on the same reduced feature set).\n",
        "\n",
        "---\n",
        "\n",
        "###  **6. When You're Avoiding Overfitting**\n",
        "\n",
        "- Because they don’t use a learning algorithm for selection, filter methods have a **lower risk of overfitting**, especially on small datasets.\n",
        "\n",
        "---\n",
        "\n",
        "###  **In Summary:**\n",
        "\n",
        "> Use **Filter methods** when you need something **fast, scalable, and simple**, especially in the early stages of your ML pipeline or when working with **very large datasets**."
      ],
      "metadata": {
        "id": "ch4nAp-3rtgX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6. In a telecom company, you are working on a project to develop a predictive model for customer churn. You are unsure of which features to include in the model because the dataset contains several different ones. Describe how you would choose the most pertinent attributes for the model using the Filter Method.\n",
        "Ans: \\\n",
        "\n",
        "In a telecom company project where you're predicting **customer churn**, the dataset might include features like:\n",
        "\n",
        "- Customer tenure  \n",
        "- Monthly charges  \n",
        "- Contract type  \n",
        "- Internet service  \n",
        "- Payment method  \n",
        "- Demographics  \n",
        "- And more...\n",
        "\n",
        "Let’s walk through **how you would use the Filter Method** to choose the most relevant features for your churn model.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Step-by-Step Process:**\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Understand Your Target Variable**\n",
        "- First, identify the target variable:  \n",
        "  ➤ Typically, it's **`Churn`** (Yes/No or 0/1)\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Separate Features by Data Type**\n",
        "- Identify whether features are:\n",
        "  - **Numerical** (e.g., Monthly Charges, Tenure)\n",
        "  - **Categorical** (e.g., Contract Type, Internet Service)\n",
        "\n",
        "This helps in choosing the right statistical test for each feature.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Apply Statistical Tests Based on Feature Type**\n",
        "\n",
        "####  **For Numerical Features:**\n",
        "- Use **Pearson correlation** or **ANOVA F-test** to measure how strongly each numeric feature relates to `Churn`.\n",
        "\n",
        "####  **For Categorical Features:**\n",
        "- Use **Chi-Square Test** or **Mutual Information** to evaluate dependency with the target.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Rank Features Based on Score**\n",
        "- Each feature will receive a score indicating its relevance.\n",
        "- Rank them from **most relevant to least relevant**.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Select Top-k Features**\n",
        "- Choose the **top N features** based on the ranking.\n",
        "- You can also set a **threshold score** and keep only those above it.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. Drop Irrelevant or Redundant Features**\n",
        "- Drop features with:\n",
        "  - Low statistical scores\n",
        "  - High correlation with each other (to reduce redundancy)\n",
        "\n",
        "---\n",
        "\n",
        "###  **Example:**\n",
        "Let’s say you evaluate your features and find:\n",
        "\n",
        "| Feature              | Chi-Square Score |\n",
        "|----------------------|------------------|\n",
        "| Contract Type        | 250              |\n",
        "| Monthly Charges      | 180              |\n",
        "| Internet Service     | 120              |\n",
        "| Tenure               | 90               |\n",
        "| Gender               | 5                |\n",
        "| Phone Service        | 4                |\n",
        "\n",
        "You might decide to **keep the top 4 features** and drop `Gender` and `Phone Service` due to low relevance.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Why Use Filter Method Here?**\n",
        "\n",
        "- The dataset likely has **many features**\n",
        "- You want a **fast and model-independent** way to reduce dimensionality\n",
        "- You're still in the **early stages** of model building"
      ],
      "metadata": {
        "id": "bv3cEFCoruCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7. You are working on a project to predict the outcome of a soccer match. You have a large dataset with many features, including player statistics and team rankings. Explain how you would use the Embedded method to select the most relevant features for the model.\n",
        "Ans: \\\n",
        "\n",
        "You're working on a project to **predict the outcome of a soccer match** using a large dataset containing:\n",
        "\n",
        "- Player stats (goals, assists, tackles, fitness, etc.)  \n",
        "- Team rankings and ratings  \n",
        "- Recent match performance  \n",
        "- Historical head-to-head data  \n",
        "- Home/away info, etc.\n",
        "\n",
        "We want to **select the most relevant features** using an **Embedded Method** — where feature selection happens **within the model training process**.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Step-by-Step Approach Using Embedded Methods:**\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Choose a Model That Supports Embedded Feature Selection**\n",
        "\n",
        "These models naturally provide **feature importance** during training:\n",
        "\n",
        "- **Lasso Regression (L1)** – if the outcome is numeric (regression)\n",
        "- **Logistic Regression with L1** – for classification (win/loss/draw)\n",
        "- **Decision Trees**, **Random Forests**, **XGBoost**, **LightGBM** – excellent for structured/tabular data\n",
        "\n",
        " Since you’re predicting match outcome (likely a classification), something like **Logistic Regression with L1** or **Random Forest/XGBoost** is ideal.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Train the Model with All Features**\n",
        "\n",
        "- Fit the model on the **full dataset** (after cleaning, encoding, etc.)\n",
        "- During training, the model will automatically **assign importance or weights** to each feature.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Extract Feature Importance**\n",
        "\n",
        "- For tree-based models (Random Forest, XGBoost, etc.):  \n",
        "  ➤ Use `.feature_importances_` to get importance scores  \n",
        "- For L1-regularized logistic regression:  \n",
        "  ➤ Check which **coefficients are zero (unimportant)** and which are non-zero (important)\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Select the Most Relevant Features**\n",
        "\n",
        "- **Set a threshold** or pick **top-k features** with the highest importance\n",
        "- You can drop features with **near-zero importance** (contribute little to the prediction)\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Retrain the Model with Selected Features**\n",
        "\n",
        "- Once irrelevant features are removed, retrain the model with the reduced set.\n",
        "- This can improve:\n",
        "  - **Training speed**\n",
        "  - **Model interpretability**\n",
        "  - **Generalization to new data (less overfitting)**\n",
        "\n",
        "---\n",
        "\n",
        "###  **Why Use Embedded Methods Here?**\n",
        "\n",
        "- You have a **large, complex dataset**\n",
        "- You want to select features **based on model performance**\n",
        "- Some features may only be useful **in combination** with others (captured well by embedded models)\n",
        "- You need a **balance of accuracy and efficiency**\n",
        "\n",
        "---\n",
        "\n",
        "###  **Example:**\n",
        "\n",
        "You train a Random Forest and extract importance:\n",
        "\n",
        "| Feature                   | Importance Score |\n",
        "|---------------------------|------------------|\n",
        "| Team FIFA Rank            | 0.21             |\n",
        "| Average Goals Scored      | 0.18             |\n",
        "| Home/Away Status          | 0.12             |\n",
        "| Player Fitness Index      | 0.11             |\n",
        "| Pass Accuracy             | 0.03             |\n",
        "| Weather Condition         | 0.01             |\n",
        "\n",
        " You decide to **keep the top 4** and drop the rest."
      ],
      "metadata": {
        "id": "gsmIOhqDruTg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8. You are working on a project to predict the price of a house based on its features, such as size, location, and age. You have a limited number of features, and you want to ensure that you select the most important ones for the model. Explain how you would use the Wrapper method to select the best set of features for the predictor.\n",
        "Ans: \\\n",
        "\n",
        "We're building a **regression model** to predict house prices using features like:\n",
        "\n",
        "- Size (square footage)  \n",
        "- Location  \n",
        "- Age of the house  \n",
        "- Number of bedrooms/bathrooms  \n",
        "- Proximity to schools or transport  \n",
        "- Year built, etc.\n",
        "\n",
        "Since you have a **limited number of features**, the **Wrapper Method** is a great choice — it evaluates feature **subsets** based on actual **model performance**, which can give you highly accurate results.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Step-by-Step Approach Using Wrapper Method:**\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Choose a Machine Learning Algorithm**\n",
        "\n",
        "Pick a **regression model** to evaluate the feature subsets, such as:\n",
        "\n",
        "- **Linear Regression**\n",
        "- **Decision Tree Regressor**\n",
        "- **Random Forest Regressor**\n",
        "\n",
        " *The wrapper method doesn't care which model you use — it wraps around any estimator to test which features work best.*\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Choose a Wrapper Strategy**\n",
        "\n",
        "There are three main strategies:\n",
        "\n",
        "- **Forward Selection:**  \n",
        "  Start with no features → add features one by one → keep the ones that improve performance\n",
        "\n",
        "- **Backward Elimination:**  \n",
        "  Start with all features → remove one feature at a time → drop the least useful ones\n",
        "\n",
        "- **Recursive Feature Elimination (RFE):**  \n",
        "  Train model → rank features by importance → remove least important → repeat until desired number is left\n",
        "\n",
        " *RFE is the most popular and often used with sklearn’s `RFE` class.*\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Split Data into Training and Testing Sets**\n",
        "\n",
        "This ensures you can evaluate feature combinations **reliably** using model performance metrics like:\n",
        "\n",
        "- Mean Squared Error (MSE)  \n",
        "- Root Mean Squared Error (RMSE)  \n",
        "- R² score\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Run the Wrapper Method**\n",
        "\n",
        "For example, using **RFE with Linear Regression**:\n",
        "\n",
        "```python\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "selector = RFE(estimator=model, n_features_to_select=5)\n",
        "selector = selector.fit(X_train, y_train)\n",
        "\n",
        "selected_features = X_train.columns[selector.support_]\n",
        "print(\"Selected features:\", selected_features)\n",
        "```\n",
        "\n",
        "This will select the **top 5 most predictive features** based on how well they help the model predict house prices.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. Train Final Model on Selected Features**\n",
        "\n",
        "After identifying the best subset, retrain your regression model **only on those features** and test its performance.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Why Wrapper Method Works Well Here:**\n",
        "\n",
        "- You have **few features**, so it’s not too computationally expensive\n",
        "- It considers **interactions between features**\n",
        "- It's **model-specific** — meaning you’re optimizing feature selection for **your chosen algorithm**\n",
        "- Can lead to **higher accuracy** than filter methods."
      ],
      "metadata": {
        "id": "onfCllywruia"
      }
    }
  ]
}