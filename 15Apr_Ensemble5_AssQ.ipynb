{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment\n",
        "\n",
        "Data Science Masters\n",
        "\n",
        "Q1. You are working on a machine learning project where you have a dataset containing numerical and\n",
        "categorical features. You have identified that some of the features are highly correlated and there are\n",
        "missing values in some of the columns. You want to build a pipeline that automates the feature\n",
        "engineering process and handles the missing values. \\\n",
        "Design a pipeline that includes the following steps\" \\\n",
        "* Use an automated feature selection method to identify the important features in the datasetC\n",
        "* Create a numerical pipeline that includes the following steps\"\n",
        "* Impute the missing values in the numerical columns using the mean of the column valuesC\n",
        "* Scale the numerical columns using standardisationC\n",
        "* Create a categorical pipeline that includes the following steps\"\n",
        "* Impute the missing values in the categorical columns using the most frequent value of the columnC\n",
        "* One-hot encode the categorical columnsC\n",
        "* Combine the numerical and categorical pipelines using a ColumnTransformerC\n",
        "* Use a Random Forest Classifier to build the final modelC\n",
        "* Evaluate the accuracy of the model on the test dataset.\n",
        "Note: Your solution should include code snippets for each step of the pipeline, and a brief explanation of\n",
        "each step. You should also provide an interpretation of the results and suggest possible improvements for\n",
        "the pipeline. \\\n",
        "Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then\n",
        "use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its\n",
        "accuracy."
      ],
      "metadata": {
        "id": "iSun_bJACAV5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1. Feature Engineering Pipeline with Random Forest Classifier**\n",
        "Ans: \\\n",
        "In this pipeline, we'll follow these steps:\n",
        "\n",
        "1. **Automated Feature Selection**: Use techniques like correlation analysis or `SelectFromModel` to identify important features.\n",
        "2. **Numerical Pipeline**:\n",
        "   - Impute missing values using the mean of the column.\n",
        "   - Standardize (scale) numerical columns.\n",
        "3. **Categorical Pipeline**:\n",
        "   - Impute missing values using the most frequent value.\n",
        "   - One-hot encode the categorical columns.\n",
        "4. **Combine Pipelines**: Use `ColumnTransformer` to combine the numerical and categorical pipelines.\n",
        "5. **Model**: Use a Random Forest Classifier.\n",
        "6. **Evaluate**: Evaluate model performance using accuracy.\n",
        "\n",
        "### **Explanation of Each Step**:\n",
        "\n",
        "1. **Automated Feature Selection**:\n",
        "   - The `SelectFromModel` method is used after fitting the Random Forest Classifier. It selects features based on feature importance derived from the model.\n",
        "   \n",
        "2. **Numerical Pipeline**:\n",
        "   - `SimpleImputer(strategy='mean')` fills in missing values with the column's mean.\n",
        "   - `StandardScaler()` standardizes the numerical columns to have a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "3. **Categorical Pipeline**:\n",
        "   - `SimpleImputer(strategy='most_frequent')` fills missing values with the most frequent value in the column.\n",
        "   - `OneHotEncoder()` one-hot encodes categorical columns, creating binary columns for each category.\n",
        "\n",
        "4. **ColumnTransformer**:\n",
        "   - Combines the two pipelines for numerical and categorical data preprocessing.\n",
        "\n",
        "5. **Model**:\n",
        "   - A Random Forest Classifier is used as the model. The `n_estimators=100` specifies 100 trees.\n",
        "\n",
        "6. **Evaluation**:\n",
        "   - The accuracy of the model is evaluated by comparing the predicted values (`y_pred`) to the true values (`y_test`).\n",
        "\n",
        "### **Possible Improvements**:\n",
        "- **Hyperparameter Tuning**: Use `GridSearchCV` or `RandomizedSearchCV` to tune hyperparameters for better model performance.\n",
        "- **Advanced Feature Selection**: Use more sophisticated methods like Recursive Feature Elimination (RFE) or feature importance from models.\n",
        "- **Ensemble Methods**: Combine other classifiers like Gradient Boosting or SVM to improve performance."
      ],
      "metadata": {
        "id": "8HFrTKkvC1vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming df is a DataFrame containing both numerical and categorical data\n",
        "df = pd.read_csv('/content/dataset.csv')\n",
        "\n",
        "# Split the data into features and target\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "# Split into train and test\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define numerical and categorical columns\n",
        "numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
        "\n",
        "# Numerical pipeline\n",
        "numerical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Impute missing values with the mean\n",
        "    ('scaler', StandardScaler())  # Standardize numerical columns\n",
        "])\n",
        "\n",
        "# Categorical pipeline\n",
        "categorical_pipeline = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Impute missing values with the most frequent value\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # One-hot encode categorical columns\n",
        "])\n",
        "\n",
        "# Column transformer to combine the pipelines\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numerical_pipeline, numerical_cols),\n",
        "    ('cat', categorical_pipeline, categorical_cols)\n",
        "])\n",
        "\n",
        "# full pipeline with feature selection and random forest classifier\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('feature_selector', SelectFromModel(RandomForestClassifier(n_estimators=100))),\n",
        "    ('classifier', RandomForestClassifier(n_estimators=100))\n",
        "])\n",
        "\n",
        "# Fit the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the model: {accuracy * 100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pm3J6VX4Cd0l",
        "outputId": "84878f2f-3219-4449-b61e-e54003e5e8d6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the model: 82.42%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Q2. Voting Classifier with Random Forest and Logistic Regression on Iris Dataset**\n",
        "\n",
        "In this pipeline, we will combine two classifiers using a Voting Classifier, which will combine their predictions:\n",
        "\n",
        "1. **Random Forest Classifier**.\n",
        "2. **Logistic Regression**.\n",
        "3. **Voting Classifier**: Combine the predictions using a voting mechanism.\n",
        "\n",
        "### **Explanation of Each Step**:\n",
        "\n",
        "1. **Classifiers**:\n",
        "   - We use `RandomForestClassifier` and `LogisticRegression` as base classifiers.\n",
        "   \n",
        "2. **Voting Classifier**:\n",
        "   - The `VotingClassifier` combines the predictions from both models. We use `voting='hard'`, which means it will take the majority vote of the two classifiers.\n",
        "\n",
        "3. **Evaluation**:\n",
        "   - The accuracy of the voting classifier is evaluated on the test set.\n",
        "\n",
        "### **Possible Improvements**:\n",
        "- **Soft Voting**: Use `voting='soft'` to predict based on the probabilities (i.e., the average of probabilities predicted by each classifier), which may improve performance.\n",
        "- **Hyperparameter Tuning**: Tune hyperparameters for both classifiers to optimize their performance."
      ],
      "metadata": {
        "id": "Q3uHVQuYDp74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize classifiers\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "lr_classifier = LogisticRegression(max_iter=200, random_state=42)\n",
        "\n",
        "# Create a voting classifier\n",
        "voting_classifier = VotingClassifier(estimators=[('rf', rf_classifier), ('lr', lr_classifier)], voting='hard')\n",
        "\n",
        "# Train the voting classifier\n",
        "voting_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = voting_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy of the Voting Classifier: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlhxAHNVD0OI",
        "outputId": "bd9684f7-90d1-438e-ef47-c048bd2517cf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the Voting Classifier: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0dinMzBSESwf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}