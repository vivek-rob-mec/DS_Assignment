{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1. What is Lasso Regression, and how does it differ from other regression techniques?**\n",
        "\n",
        "**Lasso Regression** (Least Absolute Shrinkage and Selection Operator) is a type of **linear regression** that includes a **regularization term** to prevent overfitting and improve model interpretability.\n",
        "\n",
        "- The loss function for Lasso is:\n",
        "\n",
        "$$[\n",
        "\\text{Loss} = \\text{RSS} + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
        "]$$\n",
        "\n",
        "Where:\n",
        "- RSS = Residual Sum of Squares\n",
        "- $( \\lambda )$ = regularization parameter (controls the strength of penalty)\n",
        "- $( \\beta_j )$ = coefficients of predictors\n",
        "\n",
        "---\n",
        "\n",
        "#### **How it differs from other regression techniques:**\n",
        "\n",
        "- Unlike **Ordinary Least Squares (OLS)**, Lasso includes a **penalty term** to shrink coefficients.\n",
        "- Unlike **Ridge Regression**, Lasso uses **L1 regularization** (absolute values), which can shrink some coefficients **exactly to zero**, effectively **eliminating features**.\n",
        "- This makes Lasso not only a **regression technique** but also a **feature selection method**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q2. What is the main advantage of using Lasso Regression in feature selection?**\n",
        "\n",
        "The key advantage of Lasso Regression is its ability to perform **automatic feature selection**.\n",
        "\n",
        "- Due to the **L1 penalty**, Lasso can **shrink some coefficients to exactly zero**.\n",
        "- This means Lasso **removes irrelevant or less important features** from the model entirely.\n",
        "- As a result, Lasso produces **sparse models**, which are easier to interpret and often perform better, especially when dealing with **high-dimensional data** (many features).\n",
        "\n",
        "---\n",
        "\n",
        "### **Q3. How do you interpret the coefficients of a Lasso Regression model?**\n",
        "\n",
        "In Lasso Regression:\n",
        "\n",
        "- A **non-zero coefficient** means the corresponding feature contributes to the prediction.\n",
        "- A **zero coefficient** means the feature has been **excluded** from the model (considered unimportant or redundant).\n",
        "- The magnitude and sign (positive or negative) of each **non-zero coefficient** indicate:\n",
        "  - The **strength** of the relationship.\n",
        "  - The **direction** (positive or negative effect) on the target variable.\n",
        "\n",
        "Note: Coefficients are **biased** due to the penalty — they are shrunk, so their absolute values are smaller than they would be in OLS.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?**\n",
        "\n",
        "The **main tuning parameter** in Lasso Regression is:\n",
        "\n",
        "####  **Lambda ( $( \\lambda )$ )**\n",
        "- Controls the **amount of regularization**.\n",
        "- **High $( \\lambda )$**: Strong regularization → more coefficients shrink to zero → simpler model, risk of underfitting.\n",
        "- **Low $( \\lambda )$**: Less regularization → more features kept → model might overfit.\n",
        "\n",
        "Other parameters (depending on implementation/library):\n",
        "- `max_iter`: Maximum number of iterations for optimization.\n",
        "- `tol`: Tolerance for stopping criteria.\n",
        "- `alpha` (in many libraries like scikit-learn): Equivalent to $( \\lambda )$.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?**\n",
        "\n",
        "Yes, **Lasso Regression can be used for non-linear problems**, but not **directly**.\n",
        "\n",
        "####  **How:**\n",
        "- **Transform the data** using **feature engineering** (e.g., polynomial features, interaction terms, splines).\n",
        "- Use Lasso on the transformed data to:\n",
        "  - Fit a linear model in the higher-dimensional space.\n",
        "  - Still benefit from feature selection and regularization.\n",
        "\n",
        "Alternatively:\n",
        "- Use **Lasso in combination with kernel methods** or non-linear models (e.g., in pipeline setups).\n",
        "\n",
        "> Note: Lasso is still a **linear model**, but it can approximate non-linear relationships **after transforming features**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q6. What is the difference between Ridge Regression and Lasso Regression?**\n",
        "\n",
        "| Aspect                 | Ridge Regression                         | Lasso Regression                         |\n",
        "|------------------------|-------------------------------------------|------------------------------------------|\n",
        "| Regularization Type    | L2 (squares of coefficients)              | L1 (absolute values of coefficients)      |\n",
        "| Feature Selection      | ❌ Keeps all features                     | ✅ Shrinks some coefficients to zero      |\n",
        "| Model Sparsity         | No (dense model)                         | Yes (sparse model)                        |\n",
        "| Handles Multicollinearity | ✅ Good                                | ✅ Good, but may arbitrarily choose among correlated features |\n",
        "| Best For               | Many small/medium effects                | Few strong effects                        |\n",
        "\n",
        "---\n",
        "\n",
        "### **Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?**\n",
        "\n",
        "Yes, **Lasso Regression can handle multicollinearity**, but in a **different way** than Ridge:\n",
        "\n",
        "- Lasso tends to **select only one feature** from a group of **highly correlated features**, and **set the others to zero**.\n",
        "- This leads to **simpler models**, but the choice of which feature to keep may be **somewhat arbitrary**.\n",
        "- In contrast, Ridge keeps all correlated features but **distributes the effect** among them.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?**\n",
        "\n",
        "You can choose the optimal $( \\lambda )$ using:\n",
        "\n",
        "####  **1. Cross-Validation (most common)**\n",
        "- Perform **K-fold cross-validation** to test multiple values of $( \\lambda )$.\n",
        "- Choose the value that **minimizes the cross-validation error** (e.g., MSE).\n",
        "\n",
        "####  **2. Grid Search or Random Search**\n",
        "- Use `GridSearchCV` or `RandomizedSearchCV` to test a predefined set or random range of $( \\lambda )$ values.\n",
        "\n",
        "####  **3. Information Criteria**\n",
        "- Use **AIC** or **BIC** to select $( \\lambda )$ based on model complexity and goodness of fit.\n",
        "\n",
        "####  **4. Regularization Path (LARS / Coordinate Descent)**\n",
        "- Analyze how coefficients change as $( \\lambda )$ increases.\n",
        "- Choose a balance between model sparsity and performance.\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion:**\n",
        "\n",
        "Lasso Regression is powerful for both **prediction and feature selection**, especially in high-dimensional datasets. Its ability to **shrink and eliminate** features makes it an ideal tool for interpretable, efficient modeling — but careful tuning of \\( \\lambda \\) is crucial for best performance."
      ],
      "metadata": {
        "id": "3_rjaDhCEsAb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8tLhZ9cGF6rr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}