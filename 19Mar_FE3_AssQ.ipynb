{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application.\n",
        "Ans: \\\n",
        "\n",
        "###  **Definition:**\n",
        "\n",
        "**Min-Max Scaling** (also known as **Normalization**) is a **feature scaling technique** that transforms features to a **fixed range**, typically **[0, 1]**.\n",
        "\n",
        "Itâ€™s used in data preprocessing to ensure all features contribute equally to the model by **rescaling them based on their minimum and maximum values**.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Formula:**\n",
        "$$\n",
        "[\n",
        "X_{\\text{scaled}} = \\frac{X - X_{\\text{min}}}{X_{\\text{max}} - X_{\\text{min}}}\n",
        "]\n",
        "$$\n",
        "\n",
        "\n",
        "- $( X )$ = Original feature value  \n",
        "- $( X_{\\text{min}} ), ( X_{\\text{max}} )$ = Minimum and maximum values of that feature  \n",
        "- $( X_{\\text{scaled}} \\in [0, 1] )$\n",
        "\n",
        "---\n",
        "\n",
        "###  **Why Use Min-Max Scaling?**\n",
        "\n",
        "- Some algorithms (like **KNN**, **SVM**, **Neural Networks**) are sensitive to the **scale of input features**.\n",
        "- If features have very different ranges, the model may be **biased toward the larger-scaled features**.\n",
        "- Helps models **converge faster** and perform better.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Example:**\n",
        "\n",
        "Letâ€™s say you have a feature: `House Size (in sq ft)`\n",
        "\n",
        "| Original Size |\n",
        "|---------------|\n",
        "| 1000          |\n",
        "| 1500          |\n",
        "| 2000          |\n",
        "\n",
        "**Step 1:** Identify min and max:  \n",
        "- Min = 1000  \n",
        "- Max = 2000\n",
        "\n",
        "**Step 2:** Apply Min-Max Scaling:\n",
        "$$\n",
        "[\n",
        "\\text{Scaled Size} = \\frac{\\text{Size} - 1000}{2000 - 1000}\n",
        "]\n",
        "$$\n",
        "\n",
        "| Original | Scaled   |\n",
        "|----------|----------|\n",
        "| 1000     | 0.0      |\n",
        "| 1500     | 0.5      |\n",
        "| 2000     | 1.0      |\n",
        "\n",
        "Now the sizes are all between 0 and 1 â€” ready for ML modeling.\n",
        "\n",
        "---\n",
        "\n",
        "###  **In Python (using sklearn):**\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data[['House Size']])\n",
        "```"
      ],
      "metadata": {
        "id": "7VIvq3bm1Tl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application.\n",
        "Ans: \\\n",
        "###  **Definition: Unit Vector Scaling (Normalization to Unit Norm)**\n",
        "\n",
        "The **Unit Vector technique** (also called **Vector Normalization**) is a feature scaling method where each data point (i.e., **row**) is **scaled to have a unit norm** (length = 1).  \n",
        "\n",
        "This is typically used when the **direction of the data vector** matters more than its magnitude â€” such as in **text classification**, **cosine similarity**, or **clustering** tasks.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Formula:**\n",
        "\n",
        "If a data point (row) is represented as a vector:\n",
        "$$\n",
        "[\n",
        "\\vec{x} = [x_1, x_2, ..., x_n]\n",
        "]\n",
        "$$\n",
        "Then the **normalized vector** is:\n",
        "$$\n",
        "[\n",
        "\\vec{x}_{\\text{norm}} = \\frac{\\vec{x}}{\\|\\vec{x}\\|} = \\frac{[x_1, x_2, ..., x_n]}{\\sqrt{x_1^2 + x_2^2 + ... + x_n^2}}\n",
        "]\n",
        "$$\n",
        "\n",
        "This ensures that:\n",
        "$$\n",
        "[\n",
        "\\|\\vec{x}_{\\text{norm}}\\| = 1\n",
        "]\n",
        "$$\n",
        "---\n",
        "\n",
        "###  **Difference from Min-Max Scaling:**\n",
        "\n",
        "| Aspect                | Min-Max Scaling                    | Unit Vector Scaling                        |\n",
        "|-----------------------|------------------------------------|--------------------------------------------|\n",
        "| Works on              | Individual **features (columns)**  | Individual **samples (rows)**              |\n",
        "| Output range          | Scales values to [0, 1]            | Norm (length) of each row becomes 1        |\n",
        "| Use case              | Numeric feature normalization      | Text data, cosine similarity, KNN, etc.    |\n",
        "| Preserves             | Relative spacing between values    | Direction of data vectors                  |\n",
        "\n",
        "---\n",
        "\n",
        "###  **Example:**\n",
        "\n",
        "Suppose we have two data points (rows) with 2 features:\n",
        "\n",
        "```\n",
        "X = [[3, 4],\n",
        "     [1, 2]]\n",
        "```\n",
        "\n",
        "#### â–¶ Applying **Unit Vector Scaling**:\n",
        "\n",
        "For row 1:  \n",
        "$$[\n",
        "\\sqrt{3^2 + 4^2} = \\sqrt{9 + 16} = \\sqrt{25} = 5  \n",
        "\\Rightarrow [3/5, 4/5] = [0.6, 0.8]\n",
        "]$$\n",
        "\n",
        "For row 2:  \n",
        "$$[\n",
        "\\sqrt{1^2 + 2^2} = \\sqrt{5} \\approx 2.236  \n",
        "\\Rightarrow [1/2.236, 2/2.236] \\approx [0.447, 0.894]\n",
        "]$$\n",
        "\n",
        "So the transformed matrix becomes:\n",
        "\n",
        "```\n",
        "[[0.6, 0.8],\n",
        " [0.447, 0.894]]\n",
        "```\n",
        "\n",
        "Each row now has **length = 1**.\n",
        "\n",
        "---\n",
        "\n",
        "###  **In Python (using sklearn):**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "normalizer = Normalizer()  # Uses L2 norm by default\n",
        "normalized_data = normalizer.fit_transform(X)\n",
        "``"
      ],
      "metadata": {
        "id": "iazQqfnE1Tq9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application.\n",
        "Ans: \\\n",
        "\n",
        "**Principal Component Analysis (PCA)** is a **linear dimensionality reduction** technique that transforms a high-dimensional dataset into a lower-dimensional one **while retaining as much variance (information) as possible**.\n",
        "\n",
        "It does this by finding new axes (called **principal components**) that are **orthogonal** (uncorrelated) and **ordered by the amount of variance they capture** from the data.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Why Use PCA?**\n",
        "\n",
        "- To reduce the **number of features** (dimensions) while keeping essential information\n",
        "- To **remove multicollinearity** between features\n",
        "- To improve **model training speed** and reduce **overfitting**\n",
        "- For **data visualization** (e.g., projecting data into 2D or 3D)\n",
        "\n",
        "---\n",
        "\n",
        "###  **How PCA Works (Conceptually):**\n",
        "\n",
        "1. **Standardize** the dataset (mean = 0, std = 1)\n",
        "2. **Compute the covariance matrix**\n",
        "3. **Find eigenvectors and eigenvalues** of the covariance matrix\n",
        "4. **Sort eigenvectors** by descending eigenvalues (variance explained)\n",
        "5. **Project the data** onto the top *k* eigenvectors (principal components)\n",
        "\n",
        "---\n",
        "\n",
        "###  **Example:**\n",
        "\n",
        "Suppose you have a dataset with 3 highly correlated features:  \n",
        "- `height`, `weight`, and `BMI`\n",
        "\n",
        "Instead of using all 3, you can use PCA to reduce it to **1 or 2 principal components** that still explain most of the variance.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Before PCA (3 features):**\n",
        "\n",
        "| Height | Weight | BMI |\n",
        "|--------|--------|-----|\n",
        "| 170    | 70     | 24.2|\n",
        "| 180    | 80     | 24.7|\n",
        "| 160    | 60     | 23.4|\n",
        "| ...    | ...    | ... |\n",
        "\n",
        "These are correlated â€” PCA can transform them into:\n",
        "\n",
        "---\n",
        "\n",
        "###  **After PCA (2 components):**\n",
        "\n",
        "| PC1   | PC2   |\n",
        "|-------|-------|\n",
        "| 2.34  | -0.12 |\n",
        "| 3.12  |  0.05 |\n",
        "| 1.89  | -0.08 |\n",
        "| ...   | ...   |\n",
        "\n",
        "Now you have **2 new features** (principal components), capturing most of the info with **less redundancy**.\n",
        "\n",
        "---\n",
        "\n",
        "###  **In Python (using `scikit-learn`):**\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Standardize data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Step 2: Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"Explained Variance:\", pca.explained_variance_ratio_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **Key Notes:**\n",
        "\n",
        "- PCA is **unsupervised** (doesnâ€™t use target labels)\n",
        "- Best used when features are **correlated**\n",
        "- You lose some interpretability, but gain **efficiency and performance**"
      ],
      "metadata": {
        "id": "5JUqS-mH1Tu6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept.\n",
        "Ans: \\\n",
        "\n",
        "**Feature extraction** is the process of **transforming raw data into new features** that capture the **most relevant information** for modeling â€” often reducing redundancy or dimensionality in the process.\n",
        "\n",
        "Instead of selecting features (like in feature selection), you **create new ones** that better represent the data.\n",
        "\n",
        "---\n",
        "\n",
        "###  **How PCA Relates to Feature Extraction**\n",
        "\n",
        "**PCA is a classic feature extraction technique.**\n",
        "\n",
        "- It takes a set of possibly **correlated features** and transforms them into a **new set of uncorrelated features** called **Principal Components**.\n",
        "- These principal components are **linear combinations** of the original features.\n",
        "- Each new feature (component) captures **maximum variance** from the original dataset.\n",
        "\n",
        " So, **PCA doesnâ€™t just pick features â€” it creates new ones** that summarize your data in fewer dimensions.\n",
        "\n",
        "---\n",
        "\n",
        "###  **How PCA Works for Feature Extraction**\n",
        "\n",
        "1. **Standardize the data**  \n",
        "2. **Apply PCA** to find principal components (new features)  \n",
        "3. Choose the top *k* components that explain the majority of the variance  \n",
        "4. Use these components as **input features** for your machine learning model\n",
        "\n",
        "---\n",
        "\n",
        "###  **Example:**\n",
        "\n",
        "Suppose youâ€™re predicting customer behavior based on:\n",
        "\n",
        "- Age  \n",
        "- Income  \n",
        "- Spending Score  \n",
        "- Credit Score  \n",
        "\n",
        "These may be **correlated**, and using them all might introduce **redundancy**.\n",
        "\n",
        " You apply PCA and extract **2 principal components** that capture 95% of the variance:\n",
        "\n",
        "| PC1   | PC2   |\n",
        "|-------|-------|\n",
        "| 1.23  | 0.45  |\n",
        "| 2.12  | -0.36 |\n",
        "| 0.98  | 0.12  |\n",
        "\n",
        "These new features (PC1, PC2) are used as inputs to your model â€” they are **extracted features**, not selected ones.\n",
        "\n",
        "---\n",
        "\n",
        "###  **In Python (Example):**\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assume X is your original feature set\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"Extracted Features (PCA):\", X_pca)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **In Summary:**\n",
        "\n",
        "- **Feature selection** = choose the best original features  \n",
        "- **Feature extraction** = **create new features** (like PC1, PC2) from existing ones  \n",
        "- **PCA is a feature extraction method** that transforms data into a new space of fewer, uncorrelated features with maximum variance"
      ],
      "metadata": {
        "id": "LWNYuGMa1Ty7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data.\n",
        "Ans: \\\n",
        "We're working on a **recommendation system** where the dataset has features like:\n",
        "\n",
        "- **Price** of the food item  \n",
        "- **Rating** given by customers  \n",
        "- **Delivery time** in minutes  \n",
        "\n",
        "These features are **on different scales**, and to make sure that no feature dominates the others in the model (especially in algorithms like **KNN**, **SVM**, or **clustering**), you need to **normalize** them using **Min-Max scaling**.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Why Min-Max Scaling?**\n",
        "\n",
        "- It transforms values to a common scale: **[0, 1]**\n",
        "- Makes distance-based models (like KNN or collaborative filtering) fair\n",
        "- Useful for optimizing algorithms that are sensitive to **feature magnitude**\n",
        "\n",
        "---\n",
        "\n",
        "###  **Step-by-Step Preprocessing with Min-Max Scaling:**\n",
        "\n",
        "---\n",
        "\n",
        "#### **ðŸ”¹ Original Data Example (before scaling):**\n",
        "\n",
        "| Price (â‚¹) | Rating (1â€“5) | Delivery Time (min) |\n",
        "|-----------|--------------|----------------------|\n",
        "| 150       | 4.5          | 30                   |\n",
        "| 300       | 3.0          | 50                   |\n",
        "| 200       | 5.0          | 20                   |\n",
        "\n",
        "---\n",
        "\n",
        "#### ** Apply Min-Max Scaling:**\n",
        "\n",
        "Use the formula:\n",
        "$$\n",
        "[\n",
        "X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
        "]\n",
        "$$\n",
        "#####  Step 1: Find min and max for each column:\n",
        "\n",
        "- **Price:** min = 150, max = 300  \n",
        "- **Rating:** min = 3.0, max = 5.0  \n",
        "- **Delivery Time:** min = 20, max = 50\n",
        "\n",
        "####  **Scaled Dataset:**\n",
        "\n",
        "| Price | Rating | Delivery Time |\n",
        "|-------|--------|----------------|\n",
        "| 0.00  | 0.75   | 0.33           |\n",
        "| 1.00  | 0.00   | 1.00           |\n",
        "| 0.33  | 1.00   | 0.00           |\n",
        "\n",
        "---\n",
        "\n",
        "###  **How to Do It in Python (with scikit-learn):**\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "\n",
        "# Sample data\n",
        "data = pd.DataFrame({\n",
        "    'Price': [150, 300, 200],\n",
        "    'Rating': [4.5, 3.0, 5.0],\n",
        "    'DeliveryTime': [30, 50, 20]\n",
        "})\n",
        "\n",
        "# Initialize and apply scaler\n",
        "scaler = MinMaxScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Convert back to DataFrame\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=data.columns)\n",
        "print(scaled_df)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **Summary:**\n",
        "\n",
        "- **Min-Max scaling** is essential for **equal treatment** of features with different units.\n",
        "- In this recommendation system, it ensures **fair comparisons** between price, rating, and delivery time.\n",
        "- It helps the model learn better and make more balanced suggestions."
      ],
      "metadata": {
        "id": "wcvpPOEa1T2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset.\n",
        "Ans: \\\n",
        "\n",
        "We're building a **machine learning model to predict stock prices**, and your dataset includes many features like:\n",
        "\n",
        "- Financial ratios (P/E, debt-to-equity, EPS, etc.)  \n",
        "- Historical prices  \n",
        "- Volume  \n",
        "- Market indicators  \n",
        "- Macroeconomic signals  \n",
        "\n",
        "This high-dimensional data can be **redundant**, **correlated**, and **noisy**, which can lead to:\n",
        "\n",
        "- **Overfitting**  \n",
        "- **Slower training**  \n",
        "- Difficulty in visualizing or interpreting the model  \n",
        "\n",
        "That's where **Principal Component Analysis (PCA)** comes in.\n",
        "\n",
        "---\n",
        "\n",
        "###  **How PCA Helps:**\n",
        "\n",
        "PCA reduces the number of features by transforming them into **principal components** â€” new variables that are **uncorrelated** and capture the **maximum variance** in the data.\n",
        "\n",
        "This helps by:\n",
        "- Removing multicollinearity  \n",
        "- Speeding up training  \n",
        "- Preventing overfitting  \n",
        "- Helping with feature visualization and interpretation\n",
        "\n",
        "---\n",
        "\n",
        "###  **Steps to Apply PCA in Your Stock Prediction Project:**\n",
        "\n",
        "---\n",
        "\n",
        "####  **Step 1: Standardize the Data**\n",
        "Because PCA is sensitive to scale, first **standardize all features**:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)  # X is your feature matrix\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "####  **Step 2: Apply PCA**\n",
        "\n",
        "Choose the number of principal components to keep (e.g., enough to retain 95% of the variance):\n",
        "\n",
        "```python\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.95)  # Keep components that explain 95% of the variance\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "####  **Step 3: Analyze Results**\n",
        "\n",
        "Check how much variance each principal component explains:\n",
        "\n",
        "```python\n",
        "print(pca.explained_variance_ratio_)\n",
        "print(pca.n_components_)  # Number of selected components\n",
        "```\n",
        "\n",
        "Suppose your original data had **50 features**, and PCA reduced it to **10 components** while retaining **95% of the information** â€” now your model is much more efficient.\n",
        "\n",
        "---\n",
        "\n",
        "####  **Step 4: Train the Model on Transformed Features**\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_pca, y)  # y is the target stock price\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **Before vs After PCA:**\n",
        "\n",
        "| Feature Set      | Number of Features | Multicollinearity | Training Time | Overfitting Risk |\n",
        "|------------------|--------------------|-------------------|----------------|------------------|\n",
        "| Original         | 50                 | High              | Slow           | High             |\n",
        "| After PCA        | 10â€“15              | None              | Faster         | Lower            |\n",
        "\n",
        "---\n",
        "\n",
        "###  **Summary:**\n",
        "\n",
        "- Use PCA to reduce dimensionality when working with **many financial features**  \n",
        "- It creates new features (principal components) that capture most of the variance  \n",
        "- You can improve **model performance**, reduce noise, and avoid overfitting  \n",
        "- Especially helpful in **stock prediction**, where features often overlap or correlate"
      ],
      "metadata": {
        "id": "OOiewCKB1T67"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1.\n",
        "Ans: \\\n",
        "To perform **Min-Max scaling** on the dataset `[1, 5, 10, 15, 20]` and **scale it to the range [-1, 1]**, we use the following formula:\n",
        "\n",
        "---\n",
        "\n",
        "###  **Min-Max Scaling Formula (Custom Range [a, b]):**\n",
        "$$\n",
        "[\n",
        "X_{\\text{scaled}} = a + \\frac{(X - X_{\\min}) \\times (b - a)}{X_{\\max} - X_{\\min}}\n",
        "]\n",
        "$$\n",
        "Where:\n",
        "- \\( a = -1 \\)\n",
        "- \\( b = 1 \\)\n",
        "- $( X_{\\min} = 1 )$\n",
        "- $( X_{\\max} = 20 )$\n",
        "\n",
        "---\n",
        "\n",
        "###  Apply the Formula:\n",
        "\n",
        "We'll transform each value in `[1, 5, 10, 15, 20]`.\n",
        "\n",
        "---\n",
        "\n",
        "#### 1 For X = 1:\n",
        "\n",
        "$[\n",
        "-1 + \\frac{(1 - 1) \\times (1 - (-1))}{20 - 1} = -1 + 0 = \\mathbf{-1.0}\n",
        "]\n",
        "$\n",
        "#### 2 For X = 5:\n",
        "\n",
        "$[\n",
        "-1 + \\frac{(5 - 1) \\times 2}{19} = -1 + \\frac{8}{19} â‰ˆ \\mathbf{-0.579}\n",
        "$]\n",
        "\n",
        "#### 3 For X = 10:\n",
        "\n",
        "$[\n",
        "-1 + \\frac{(10 - 1) \\times 2}{19} = -1 + \\frac{18}{19} â‰ˆ \\mathbf{-0.053}\n",
        "$]\n",
        "\n",
        "#### 4 For X = 15:\n",
        "\n",
        "$[\n",
        "-1 + \\frac{(15 - 1) \\times 2}{19} = -1 + \\frac{28}{19} â‰ˆ \\mathbf{0.474}\n",
        "$]\n",
        "\n",
        "#### 5 For X = 20:\n",
        "\n",
        "$[\n",
        "-1 + \\frac{(20 - 1) \\times 2}{19} = -1 + \\frac{38}{19} = -1 + 2 = \\mathbf{1.0}\n",
        "$]\n",
        "\n",
        "---\n",
        "\n",
        "### **Scaled Output (Range -1 to 1):**\n",
        "\n",
        "| Original | Scaled   |\n",
        "|----------|----------|\n",
        "| 1        | -1.000   |\n",
        "| 5        | -0.579   |\n",
        "| 10       | -0.053   |\n",
        "| 15       | 0.474    |\n",
        "| 20       | 1.000    "
      ],
      "metadata": {
        "id": "0wyKHBkq1T-6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
        "Ans: \\\n",
        "\n",
        "###  **Step 1: Understand the Dataset**\n",
        "\n",
        "You have **5 features**:\n",
        "\n",
        "- **Height** â€“ continuous  \n",
        "- **Weight** â€“ continuous  \n",
        "- **Age** â€“ continuous  \n",
        "- **Gender** â€“ categorical (likely binary: male/female or 0/1)  \n",
        "- **Blood Pressure** â€“ continuous  \n",
        "\n",
        " So, 4 numerical + 1 categorical (gender), which must be **encoded numerically** for PCA.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Step 2: Preprocess the Data**\n",
        "\n",
        "- **Standardize** all numeric features (mean = 0, std = 1)\n",
        "- **Encode** categorical variables (e.g., one-hot or label encoding for gender)\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame (assuming gender already encoded as 0/1)\n",
        "data = pd.DataFrame({\n",
        "    'height': [...],\n",
        "    'weight': [...],\n",
        "    'age': [...],\n",
        "    'gender': [...],\n",
        "    'blood_pressure': [...]\n",
        "})\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Apply PCA\n",
        "pca = PCA()\n",
        "pca_data = pca.fit_transform(scaled_data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **Step 3: Decide How Many Principal Components to Retain**\n",
        "\n",
        "Use **explained variance ratio** to determine how many components to keep:\n",
        "\n",
        "```python\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
        "```\n",
        "\n",
        "Then plot or check the cumulative variance:\n",
        "\n",
        "| PC # | Variance Explained | Cumulative Variance |\n",
        "|------|--------------------|---------------------|\n",
        "| PC1  | 0.45               | 0.45                |\n",
        "| PC2  | 0.25               | 0.70                |\n",
        "| PC3  | 0.15               | 0.85                |\n",
        "| PC4  | 0.10               | 0.95                |\n",
        "| PC5  | 0.05               | 1.00                |\n",
        "\n",
        "---\n",
        "\n",
        "###  **How Many Principal Components to Choose?**\n",
        "\n",
        " Choose **enough components to retain ~95%** of the variance.\n",
        "\n",
        "So in this case:\n",
        "- Retaining **4 components** keeps **95% of the variance**\n",
        "- Thatâ€™s a good trade-off between dimensionality reduction and information retention\n",
        "\n",
        "---\n",
        "\n",
        "###  **Final PCA Application:**\n",
        "\n",
        "```python\n",
        "pca = PCA(n_components=4)  # Retain 95% variance\n",
        "reduced_data = pca.fit_transform(scaled_data)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **Summary:**\n",
        "\n",
        "- PCA helps compress 5 features into **~4 principal components**  \n",
        "- You reduce redundancy and keep essential patterns in the data  \n",
        "- The number of components is chosen based on how much **variance you want to retain** (commonly â‰¥ 95%)"
      ],
      "metadata": {
        "id": "wgettpk51UEX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_ohWHrVxBAeH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}