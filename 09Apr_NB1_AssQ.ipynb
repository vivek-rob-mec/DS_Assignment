{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "###  **Q1. What is Bayes' Theorem?**\n",
        "\n",
        "**Bayes’ Theorem** is a mathematical rule that allows us to **update the probability of an event** based on new evidence. It connects **prior knowledge** with **new data**, helping us make better predictions and decisions.\n",
        "\n",
        "It’s widely used in statistics, machine learning, medicine, and more.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Q2. What is the formula for Bayes' Theorem?**\n",
        "\n",
        "The mathematical formula is:\n",
        "\n",
        "$$[\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "]$$\n",
        "\n",
        "Where:\n",
        "- **\\(P(A|B)\\)**: Posterior probability – the updated probability of event **A** after observing **B**\n",
        "- **\\(P(B|A)\\)**: Likelihood – the probability of observing **B** given that **A** is true\n",
        "- **\\(P(A)\\)**: Prior probability – our belief about **A** before seeing the evidence\n",
        "- **\\(P(B)\\)**: Evidence – the total probability of observing **B**\n",
        "\n",
        "---\n",
        "\n",
        "###  **Q3. How is Bayes’ Theorem used in practice?**\n",
        "\n",
        "Bayes' Theorem is applied in many real-world scenarios:\n",
        "\n",
        "####  **1. Medical Diagnosis**\n",
        "- Estimate the probability that a patient has a disease given a positive test result.\n",
        "- Helps adjust for **false positives** and **rare conditions**.\n",
        "\n",
        "####  **2. Spam Detection**\n",
        "- In **Naive Bayes classifiers**, it's used to determine if an email is spam based on the frequency of words like “buy now” or “free.”\n",
        "\n",
        "####  **3. Machine Learning & AI**\n",
        "- Bayesian networks, probabilistic models, and classification tasks (e.g., text classification).\n",
        "\n",
        "####  **4. Risk Assessment**\n",
        "- In finance or engineering, it’s used to update risk levels when new information becomes available.\n",
        "\n",
        "####  **5. Legal Reasoning**\n",
        "- Used to evaluate the probability of guilt given new evidence in forensic analysis.\n",
        "\n",
        "---\n",
        "\n",
        "###  Example:\n",
        "Imagine you're testing for a rare disease (1% prevalence), and the test is 99% accurate.\n",
        "\n",
        "Even if you test **positive**, Bayes’ Theorem shows your actual chance of having the disease is much lower than 99% — because false positives matter more when the disease is rare.\n",
        "\n",
        "\n",
        "###  **Example: Medical Diagnosis**\n",
        "Let’s say:\n",
        "- 1% of people have a rare disease → \\(P(Disease) = 0.01\\)\n",
        "- A test detects it 99% of the time when it's there → \\(P(Positive|Disease) = 0.99\\)\n",
        "- But the test gives false positives 5% of the time → \\(P(Positive|No Disease) = 0.05\\)\n",
        "\n",
        "You test positive. What’s the chance you actually have the disease?\n",
        "\n",
        "Using Bayes' Theorem:\n",
        "\n",
        "$$[\n",
        "P(Disease|Positive) = \\frac{P(Positive|Disease) \\cdot P(Disease)}{P(Positive)}\n",
        "]$$\n",
        "\n",
        "$$[\n",
        "P(Positive) = P(Positive|Disease) \\cdot P(Disease) + P(Positive|No Disease) \\cdot P(No Disease)\n",
        "]$$\n",
        "\n",
        "$$[\n",
        "= (0.99 \\cdot 0.01) + (0.05 \\cdot 0.99) = 0.0594\n",
        "]$$\n",
        "\n",
        "$$[\n",
        "P(Disease|Positive) = \\frac{0.0099}{0.0594} \\approx 0.1667\n",
        "]$$\n",
        "\n",
        " So even if you test positive, there’s only a **16.7%** chance you actually have the disease."
      ],
      "metadata": {
        "id": "j5C2tsY4t2UA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. What is the relationship between Bayes' theorem and conditional probability?\n",
        "Ans: \\\n",
        "\n",
        "Bayes' Theorem is **built directly on the concept of conditional probability** — in fact, it's a **rearrangement** of the formula for conditional probability.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Conditional Probability Formula:**\n",
        "The probability of event **A** given event **B** is:\n",
        "\n",
        "$$[\n",
        "P(A|B) = \\frac{P(A \\cap B)}{P(B)}\n",
        "]$$\n",
        "\n",
        "This tells us the probability of **A** happening if we know that **B** has happened.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Bayes' Theorem Derived from Conditional Probability:**\n",
        "\n",
        "From the definition of conditional probability:\n",
        "\n",
        "$$[\n",
        "P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)}\n",
        "]$$\n",
        "\n",
        "This is **Bayes’ Theorem** — just a restructured version of conditional probability that **flips the condition** from \\(P(B|A)\\) to \\(P(A|B)\\), allowing us to **reverse** our understanding based on new evidence.\n",
        "\n",
        "---\n",
        "\n",
        "###  **In Simple Terms:**\n",
        "- **Conditional probability** tells us: \"Given B, what's the chance of A?\"\n",
        "- **Bayes' Theorem** tells us: \"We know how likely A is, and how likely B is if A is true — so what’s the chance A is true now that B has happened?\"\n",
        "\n",
        "---\n",
        "\n",
        "###  Real-World Analogy:\n",
        "Think of a weather app:\n",
        "- Conditional probability might tell you: \"If it's raining, there's a 90% chance clouds were present.\"\n",
        "- Bayes' Theorem helps you reverse it: \"It’s cloudy now. What’s the chance it’s raining?\""
      ],
      "metadata": {
        "id": "Ld6i7jnZt2h2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?\n",
        "Ans: \\\n",
        "\n",
        "There are **three main types** of Naive Bayes classifiers in practice, and the choice depends on the **nature of your features (input variables)**.\n",
        "\n",
        "---\n",
        "\n",
        "###  1. **Gaussian Naive Bayes**  \n",
        "Use when:\n",
        "- Features are **continuous** (real numbers).\n",
        "- Data is assumed to follow a **normal (Gaussian) distribution**.\n",
        "\n",
        " Example use case:\n",
        "- Predicting whether a patient has a disease based on continuous variables like age, BMI, and blood pressure.\n",
        "\n",
        " Assumes:\n",
        "$$[\n",
        "P(x_i | y) \\sim \\mathcal{N}(\\mu_y, \\sigma_y^2)\n",
        "]$$\n",
        "\n",
        "---\n",
        "\n",
        "###  2. **Multinomial Naive Bayes**  \n",
        "Use when:\n",
        "- Features are **discrete counts** (e.g., word frequencies in text).\n",
        "- Mostly used for **text classification**, like spam filtering or document categorization.\n",
        "\n",
        " Example use case:\n",
        "- Classifying emails as spam/ham based on word counts.\n",
        "\n",
        " Assumes:\n",
        "- Features represent the number of times a word appears in a document.\n",
        "\n",
        "---\n",
        "\n",
        "###  3. **Bernoulli Naive Bayes**  \n",
        "Use when:\n",
        "- Features are **binary** (0 or 1), indicating the **presence or absence** of something.\n",
        "\n",
        " Example use case:\n",
        "- Sentiment analysis using binary indicators for whether specific words appear in a tweet.\n",
        "\n",
        " Assumes:\n",
        "- Features follow a **Bernoulli distribution** (only 0 or 1 values).\n",
        "\n",
        "---\n",
        "\n",
        "###  Summary Table:\n",
        "\n",
        "| Classifier Type      | Use When Features Are         | Common Use Case                  |\n",
        "|----------------------|-------------------------------|----------------------------------|\n",
        "| Gaussian NB          | Continuous (real-valued)      | Medical prediction, sensors      |\n",
        "| Multinomial NB       | Discrete counts (frequencies) | Text classification, spam        |\n",
        "| Bernoulli NB         | Binary (0 or 1)               | Presence/absence of features     |\n",
        "\n",
        "---\n",
        "\n",
        "###  Bonus Tip:\n",
        "If you're unsure, look at your dataset:\n",
        "- Are your features numbers? Try **Gaussian**.\n",
        "- Are they counts? Try **Multinomial**.\n",
        "- Are they 0/1 indicators? Go with **Bernoulli**.\n"
      ],
      "metadata": {
        "id": "urqmb0tTt2uB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment:\n",
        "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of each feature value for each class: \\\n",
        "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4 \\\n",
        "A 3 3 4 4 3 3 3 \\\n",
        "B 2 2 1 2 2 2 3 \\\n",
        "Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
        "to belong to?\n",
        "Ans: \\\n",
        "To solve this problem using **Naive Bayes**, we need to compute the **posterior probabilities** for each class (A and B) given the new instance with **X1 = 3** and **X2 = 4**. Since we are assuming **equal prior probabilities** for each class, the prior probability $$( P(A) = P(B) = 0.5 )$$.\n",
        "\n",
        "### Step-by-Step Calculation:\n",
        "\n",
        "1. **Prior probabilities:**\n",
        "   Since we assume equal prior probabilities:\n",
        "   $$[\n",
        "   P(A) = 0.5, \\quad P(B) = 0.5\n",
        "   ]$$\n",
        "\n",
        "2. **Likelihoods:**\n",
        "   We need to calculate the likelihoods for each class. This is based on the frequency table for each feature value given the class.\n",
        "\n",
        "#### Class A:\n",
        "- $$( P(X1=3 | A) = \\frac{4}{3 + 3 + 4} = \\frac{4}{10} )$$\n",
        "- $$( P(X2=4 | A) = \\frac{3}{4 + 3 + 3 + 3} = \\frac{3}{13} )$$\n",
        "\n",
        "#### Class B:\n",
        "- $$( P(X1=3 | B) = \\frac{1}{2 + 2 + 1} = \\frac{1}{5} )$$\n",
        "- $$( P(X2=4 | B) = \\frac{3}{2 + 2 + 2 + 3} = \\frac{3}{9} )$$\n",
        "\n",
        "3. **Posterior probabilities:**\n",
        "\n",
        "Using Bayes’ Theorem, the **posterior probability** for each class is:\n",
        "\n",
        "$$[\n",
        "P(A | X1 = 3, X2 = 4) \\propto P(X1 = 3 | A) \\cdot P(X2 = 4 | A) \\cdot P(A)\n",
        "]$$\n",
        "$$[\n",
        "P(B | X1 = 3, X2 = 4) \\propto P(X1 = 3 | B) \\cdot P(X2 = 4 | B) \\cdot P(B)\n",
        "]$$\n",
        "\n",
        "For **Class A**:\n",
        "\n",
        "$$[\n",
        "P(A | X1 = 3, X2 = 4) \\propto \\frac{4}{10} \\cdot \\frac{3}{13} \\cdot 0.5\n",
        "]$$\n",
        "$$[\n",
        "P(A | X1 = 3, X2 = 4) \\propto \\frac{4 \\times 3}{10 \\times 13} \\times 0.5 = \\frac{12}{130} \\times 0.5 = \\frac{6}{130} = 0.0462\n",
        "]$$\n",
        "\n",
        "For **Class B**:\n",
        "\n",
        "$$[\n",
        "P(B | X1 = 3, X2 = 4) \\propto \\frac{1}{5} \\cdot \\frac{3}{9} \\cdot 0.5\n",
        "]$$\n",
        "$$[\n",
        "P(B | X1 = 3, X2 = 4) \\propto \\frac{1 \\times 3}{5 \\times 9} \\times 0.5 = \\frac{3}{45} \\times 0.5 = \\frac{1.5}{45} = 0.0333\n",
        "]$$\n",
        "\n",
        "4. **Conclusion:**\n",
        "Since the posterior probability for **Class A** (0.0462) is greater than the posterior probability for **Class B** (0.0333), the **Naive Bayes classifier** would predict that the new instance with **X1 = 3** and **X2 = 4** belongs to **Class A**."
      ],
      "metadata": {
        "id": "7xec840pt263"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Given frequency table\n",
        "# Class A and B frequency for X1 and X2 values\n",
        "class_A = {1: 3, 2: 3, 3: 4, 4: 3}  # Frequency of X1 for Class A, Frequency of X2 for Class A\n",
        "class_B = {1: 2, 2: 2, 3: 1, 4: 3}  # Frequency of X1 for Class B, Frequency of X2 for Class B\n",
        "\n",
        "# Total counts for each class (sums of frequencies for each feature)\n",
        "total_A = sum(class_A.values())  # Total instances in Class A\n",
        "total_B = sum(class_B.values())  # Total instances in Class B\n",
        "\n",
        "# Prior probabilities (equal priors)\n",
        "P_A = 0.5\n",
        "P_B = 0.5\n",
        "\n",
        "# Likelihoods for Class A and Class B given the features X1 = 3, X2 = 4\n",
        "P_X1_given_A = class_A[3] / total_A\n",
        "P_X2_given_A = class_A[4] / total_A\n",
        "\n",
        "P_X1_given_B = class_B[3] / total_B\n",
        "P_X2_given_B = class_B[4] / total_B\n",
        "\n",
        "# Compute posterior probabilities using Bayes' Theorem\n",
        "P_A_given_X = P_X1_given_A * P_X2_given_A * P_A\n",
        "P_B_given_X = P_X1_given_B * P_X2_given_B * P_B\n",
        "\n",
        "# Output the results\n",
        "print(f\"Posterior probability for Class A: {P_A_given_X}\")\n",
        "print(f\"Posterior probability for Class B: {P_B_given_X}\")\n",
        "\n",
        "# Class prediction based on the higher posterior probability\n",
        "if P_A_given_X > P_B_given_X:\n",
        "    print(\"Predicted Class: A\")\n",
        "else:\n",
        "    print(\"Predicted Class: B\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRBXXv8YzBdQ",
        "outputId": "d3f98585-6a1c-4f30-c3b7-36c0039ff4a7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Posterior probability for Class A: 0.03550295857988166\n",
            "Posterior probability for Class B: 0.0234375\n",
            "Predicted Class: A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-yxrYoOYzCen"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}