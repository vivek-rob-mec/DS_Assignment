{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSgTYJeKj_5s"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Q1. In order to predict house price based on several characteristics, such as location, square footage,\n",
        "number of bedrooms, etc., you are developing an SVM regression model. Which regression metric in this\n",
        "situation would be the best to employ?\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  **Best Regression Metrics to Use:**\n",
        "\n",
        "#### **1. Mean Absolute Error (MAE)**\n",
        "- **Definition**: Average of the absolute differences between predicted and actual values.\n",
        "- **Why it's good**: Easy to interpret (in same units as house price), **less sensitive to outliers** than MSE.\n",
        "- **Use case**: When you want to know the average prediction error in real currency terms.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **2. Mean Squared Error (MSE)**\n",
        "- **Definition**: Average of squared differences between predictions and actual values.\n",
        "- **Why it's good**: Penalizes large errors more (useful if large errors are costly).\n",
        "- **Use case**: When large prediction errors are especially undesirable.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import mean_squared_error\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **3. Root Mean Squared Error (RMSE)**\n",
        "- **Definition**: Square root of MSE.\n",
        "- **Why it's good**: Keeps units consistent with the target (house price), still penalizes large errors.\n",
        "- **Use case**: Very commonly used in real estate models.\n",
        "\n",
        "```python\n",
        "rmse = mean_squared_error(y_true, y_pred, squared=False)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### **4. R-squared (Coefficient of Determination)**\n",
        "- **Definition**: Proportion of variance in the dependent variable that is predictable from the features.\n",
        "- **Why it's good**: Tells how well your model explains variability in house prices.\n",
        "- **Use case**: Good for understanding overall fit, but **not ideal for comparing individual errors**.\n",
        "\n",
        "```python\n",
        "from sklearn.metrics import r2_score\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **Recommended Metric for House Price Prediction**\n",
        "-  **MAE** is generally the best metric for house price predictions if you want an interpretable average error.\n",
        "-  Use **RMSE** if large price errors are more problematic (e.g., over/under-pricing houses).\n",
        "- ℹ **R²** is good for comparing models, but not always reliable alone."
      ],
      "metadata": {
        "id": "sVBGlw4LpRJ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. You have built an SVM regression model and are trying to decide between using MSE or R-squared as your evaluation metric. Which metric would be more appropriate if your goal is to predict the actual price of a house as accurately as possible?\n",
        "Ans: \\\n",
        "###  **Use Mean Squared Error (MSE)**\n",
        "\n",
        "####  Why MSE is more appropriate:\n",
        "- MSE measures the **average squared difference** between the predicted prices and the actual prices.\n",
        "- It directly tells you **how far off your predictions are**, on average, from the true values (emphasizing larger errors).\n",
        "- Since you're focused on **accuracy of actual price values**, this is exactly what MSE captures.\n",
        "\n",
        "---\n",
        "\n",
        "###  Why not just R-squared:\n",
        "- R² measures **how well the model explains variance**, but it **doesn’t tell you how close** your predictions are to the actual price.\n",
        "- You could have a high R² and still have large errors in prediction if the scale of predictions is off.\n",
        "- It's better for **model comparison**, not for evaluating **absolute accuracy**.\n",
        "\n",
        "---\n",
        "\n",
        "###  Conclusion:\n",
        ">  **Use MSE (or RMSE)** if your goal is **price accuracy in real numbers**.  \n",
        "> Use R² **in addition** to see how well your model explains the data, but **not alone** for price accuracy."
      ],
      "metadata": {
        "id": "uYY6ts6ap39v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. You have a dataset with a significant number of outliers and are trying to select an appropriate regression metric to use with your SVM model. Which metric would be the most appropriate in this scenario?\n",
        "Ans: \\\n",
        "\n",
        "###  **Mean Absolute Error (MAE)**\n",
        "\n",
        "---\n",
        "\n",
        "###  Why MAE is the best choice:\n",
        "- **MAE** calculates the average of the **absolute errors** between predicted and actual values.\n",
        "- It **treats all errors equally**, meaning **outliers won't be excessively penalized**.\n",
        "- Unlike **MSE**, it does **not square** the errors, so extreme values (outliers) have **less impact** on the overall error.\n",
        "\n",
        "---\n",
        "\n",
        "### Why not MSE or RMSE:\n",
        "- **MSE** and **RMSE** **square the errors**, which gives **extra weight** to large errors caused by outliers.\n",
        "- These metrics tend to **overemphasize outliers**, potentially distorting your model evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "###  Summary of Metric Behavior with Outliers:\n",
        "\n",
        "| Metric | Sensitive to Outliers? | Suitable for Outlier-Rich Data? |\n",
        "|--------|------------------------|-------------------------------|\n",
        "| MAE    | ❌ No                   | ✅ Yes                         |\n",
        "| MSE    | ✅ Yes                  | ❌ No                          |\n",
        "| RMSE   | ✅ Yes (but interpretable) | ❌ No                     |\n",
        "| R²     | ✅ Yes (indirectly)      | ❌ No (may drop sharply)       |\n",
        "\n",
        "---\n",
        "\n",
        "###  Conclusion:\n",
        ">  **Use MAE** when your dataset contains outliers, and you want a **robust, balanced evaluation** of your regression model."
      ],
      "metadata": {
        "id": "dmwAyisxq1U3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NsPSf7J3qwEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. You have built an SVM regression model using a polynomial kernel and are trying to select the best metric to evaluate its performance. You have calculated both MSE and RMSE and found that both values are very close. Which metric should you choose to use in this case?\n",
        "Ans: \\\n",
        "\n",
        "###  **Will choose RMSE (Root Mean Squared Error)**\n",
        "\n",
        "####  Why?\n",
        "- **RMSE is in the same unit** as the target variable (e.g., dollars, square feet, etc.).\n",
        "- It's **easier to interpret** than MSE, which is in **squared units**.\n",
        "- When MSE and RMSE are close, it simply means your errors are consistent and not too large—so RMSE offers a clearer, more human-readable number.\n",
        "\n",
        "---\n",
        "\n",
        "###  Quick Comparison:\n",
        "\n",
        "| Metric | Unit         | Interpretability | Penalizes Large Errors |\n",
        "|--------|--------------|------------------|-------------------------|\n",
        "| MSE    | Squared unit | ❌ Hard to read   | ✅ Strongly              |\n",
        "| RMSE   | Original unit| ✅ Easy to read   | ✅ Strongly              |\n",
        "\n",
        "---\n",
        "\n",
        "###  Conclusion:\n",
        ">  Use **RMSE** when both values are close, because it's **more intuitive** and helps **communicate results better** to stakeholders (e.g., \"our model predicts house prices with an average error of $4,000\")."
      ],
      "metadata": {
        "id": "TWvHHAsSr70B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. You are comparing the performance of different SVM regression models using different kernels (linear, polynomial, and RBF) and are trying to select the best evaluation metric. Which metric would be most appropriate if your goal is to measure how well the model explains the variance in the target variable?\n",
        "Ans: \\\n",
        "\n",
        "###  **R-squared (R² Score)** – also known as the **coefficient of determination**\n",
        "\n",
        "---\n",
        "\n",
        "###  Why R² is the right choice:\n",
        "- R² tells you the **proportion of the variance** in the target variable that is **explained by your model**.\n",
        "- It is a **relative measure** — perfect for **comparing models** using different kernels.\n",
        "- R² ranges from:\n",
        "  - **1.0** → perfect prediction\n",
        "  - **0.0** → model does no better than predicting the mean\n",
        "  - **< 0.0** → model is worse than predicting the mean\n",
        "\n",
        "---\n",
        "\n",
        "###  Example:\n",
        "Let’s say you're comparing models:\n",
        "\n",
        "| Kernel      | R² Score |\n",
        "|-------------|----------|\n",
        "| Linear      | 0.72     |\n",
        "| Polynomial  | 0.84     |\n",
        "| RBF         | 0.87     |\n",
        "\n",
        "Here, the **RBF kernel** explains the most variance in the target — so it's the best fit **in terms of capturing the pattern in the data**.\n",
        "\n",
        "---\n",
        "\n",
        "###  When **not** to use R²:\n",
        "- If you're concerned with **absolute accuracy** (e.g., exact price predictions), then **MAE or RMSE** would be better.\n",
        "- R² doesn't tell you how **far off** the predictions are — only how **well the model captures variance**.\n",
        "\n",
        "---\n",
        "\n",
        "###  Conclusion:\n",
        "> If your **goal is to measure explanatory power**, go with **R² score** — it’s ideal for comparing different kernel models."
      ],
      "metadata": {
        "id": "zRLO0baxsf3g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "geJzhZoFtI82"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}