{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is Gradient Boosting Regression?\n",
        "Ans: \\\n",
        "**Gradient Boosting Regression (GBR)** is a machine learning algorithm used for **regression tasks** (predicting continuous values) by combining multiple weak learners (typically decision trees) into a **strong model**.\n",
        "\n",
        "Gradient Boosting is an ensemble learning method that builds a series of models sequentially, where each new model tries to correct the errors (residuals) of the previous ones. It is called \"gradient\" boosting because it **minimizes the residuals using gradient descent**.\n",
        "\n",
        "In **Gradient Boosting Regression**, the aim is to predict continuous outcomes, for example, house prices or stock prices.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Working:**\n",
        "\n",
        "1. **Initialization:**\n",
        "   - Start with an initial prediction (usually the mean of the target values in the training set).\n",
        "\n",
        "2. **Sequential Learning:**\n",
        "   - In each iteration, a new weak learner (typically a decision tree) is added to the model.\n",
        "   - The model **focuses on the residuals (errors)** of the previous iteration.\n",
        "   - The residuals represent how far off the model’s previous predictions were from the true values.\n",
        "\n",
        "3. **Gradient Descent on Residuals:**\n",
        "   - In each round, the algorithm **fits a model to the residuals** using a gradient descent approach to minimize the loss function.\n",
        "   - The new model is added to the ensemble with a weight (learning rate) that determines how much influence it has.\n",
        "\n",
        "4. **Update the Prediction:**\n",
        "   - The final prediction is the sum of the initial prediction and all the contributions from the subsequent weak learners.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Mathematical Formulation:**\n",
        "\n",
        "For a regression task, the gradient boosting model predicts the target \\( y \\) as:\n",
        "\n",
        "$$[\n",
        "F(x) = F_0 + \\sum_{m=1}^{M} \\gamma_m h_m(x)\n",
        "]$$\n",
        "\n",
        "Where:\n",
        "- \\( F(x) \\) is the final prediction\n",
        "- $( F_0 $) is the initial prediction (mean of the target values)\n",
        "- $( h_m(x) $) is the weak learner (decision tree) at iteration \\( m \\)\n",
        "- $( \\gamma_m $) is the weight of the weak learner at iteration \\( m \\)\n",
        "- $( M $) is the total number of iterations (trees)\n",
        "\n",
        "The **goal** of each iteration is to **minimize the residuals** (the difference between true values and the predicted values) using a gradient-based optimization approach.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Loss Function for Regression:**\n",
        "\n",
        "The **loss function** in Gradient Boosting Regression is typically the **mean squared error (MSE)** for regression tasks:\n",
        "\n",
        "$$[\n",
        "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(y_i - F(x_i)\\right)^2\n",
        "]$$\n",
        "\n",
        "Where:\n",
        "- $( y_i )$ is the true target value for sample \\( i \\)\n",
        "- $( F(x_i) )$ is the predicted value for sample \\( i \\)\n",
        "\n",
        "This loss function measures the difference between the predicted and true values, and the algorithm aims to minimize it through gradient descent.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Key Characteristics of Gradient Boosting Regression:**\n",
        "\n",
        "1. **Sequential Learning:**\n",
        "   - Each new tree is trained on the residuals of the previous model.\n",
        "   \n",
        "2. **Ensemble of Weak Learners:**\n",
        "   - Uses decision trees as weak learners, but combined together, they create a powerful model.\n",
        "\n",
        "3. **Gradient Descent Optimization:**\n",
        "   - The algorithm uses **gradient descent** to minimize the loss function and update the model with each iteration.\n",
        "\n",
        "4. **Overfitting Risk:**\n",
        "   - Gradient Boosting can overfit if the number of iterations is too high or if the trees are too deep. Proper tuning (e.g., adjusting the learning rate, tree depth) is needed.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Advantages:**\n",
        "\n",
        "- **Highly accurate**: It typically outperforms other regression models.\n",
        "- **Flexibility**: Can handle various loss functions and types of data.\n",
        "- **Handles missing data well**.\n",
        "\n",
        "###  **Limitations:**\n",
        "\n",
        "- **Sensitive to noise**: Can overfit noisy data.\n",
        "- **Computationally expensive**: Requires more time for training compared to simpler models.\n",
        "- **Requires careful hyperparameter tuning** (e.g., learning rate, number of trees).\n",
        "\n",
        "---\n",
        "\n",
        "###  **Summary:**\n",
        "Gradient Boosting Regression is a powerful, **sequential ensemble method** that uses gradient descent to optimize the model by fitting weak learners to the residuals of the previous learners, ultimately building a robust predictive model."
      ],
      "metadata": {
        "id": "ZwmFsmtiCEQa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "id": "-TB82oJcB11P",
        "outputId": "fc3a279a-4274-42f0-e583-0a4b35b91179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/fromnumeric.py:3596: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.11/dist-packages/numpy/_core/_methods.py:138: RuntimeWarning: invalid value encountered in scalar divide\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training MSE: 34.648841765787424\n",
            "Test MSE: 36.68854719367578\n",
            "Training R-squared: 0.0\n",
            "Test R-squared: 0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\"\"\"\n",
        "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a\n",
        "simple regression problem as an example and train the model on a small dataset. Evaluate the model's\n",
        "performance using metrics such as mean squared error and R-squared.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Generate a simple regression dataset\n",
        "np.random.seed(42)\n",
        "\n",
        "# Generate some data: y = 2 * x + 1 + noise\n",
        "X = np.random.rand(100, 1) * 10  # 100 data points, feature x between 0 and 10\n",
        "y = 2 * X + 1 + np.random.randn(100, 1) * 2  # Linear relation with noise\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 2: Implement Gradient Boosting Algorithm\n",
        "\n",
        "# Define the weak learner: A simple decision stump (tree with a single split)\n",
        "def decision_stump(X, y):\n",
        "    best_split = None\n",
        "    min_error = float('inf')\n",
        "\n",
        "    for threshold in np.unique(X):  # Try all possible thresholds\n",
        "        # Use element-wise comparison and broadcasting to create predictions\n",
        "        predictions = np.where(threshold > X, np.mean(y[threshold > X]), np.mean(y[threshold <= X]))\n",
        "        error = np.sum((predictions - y) ** 2)\n",
        "        if error < min_error:\n",
        "            min_error = error\n",
        "            best_split = threshold\n",
        "    return best_split\n",
        "\n",
        "# Gradient Boosting algorithm from scratch\n",
        "def gradient_boosting(X, y, n_estimators=100, learning_rate=0.1):\n",
        "    # Initialize the model with the mean value of the target\n",
        "    F = np.ones_like(y) * np.mean(y)\n",
        "\n",
        "    for m in range(n_estimators):\n",
        "        # Calculate residuals (errors)\n",
        "        residuals = y - F\n",
        "\n",
        "        # Fit a weak learner (decision stump) on the residuals\n",
        "        stump = decision_stump(X, residuals)\n",
        "\n",
        "        # Update the model by adding the weak learner’s prediction, scaled by the learning rate\n",
        "        F += learning_rate * (X > stump) * (np.mean(residuals))\n",
        "\n",
        "    return F\n",
        "\n",
        "# Step 3: Train the Gradient Boosting model\n",
        "n_estimators = 100\n",
        "learning_rate = 0.1\n",
        "y_pred_train = gradient_boosting(X_train, y_train, n_estimators, learning_rate)\n",
        "y_pred_test = gradient_boosting(X_test, y_test, n_estimators, learning_rate)\n",
        "\n",
        "# Step 4: Evaluate the model using MSE and R-squared\n",
        "mse_train = mean_squared_error(y_train, y_pred_train)\n",
        "mse_test = mean_squared_error(y_test, y_pred_test)\n",
        "\n",
        "r2_train = r2_score(y_train, y_pred_train)\n",
        "r2_test = r2_score(y_test, y_pred_test)\n",
        "\n",
        "print(f\"Training MSE: {mse_train}\")\n",
        "print(f\"Test MSE: {mse_test}\")\n",
        "print(f\"Training R-squared: {r2_train}\")\n",
        "print(f\"Test R-squared: {r2_test}\")\n",
        "\n",
        "# Plot the predicted vs actual values for visualization\n",
        "plt.scatter(X_test, y_test, color='black', label='True values')\n",
        "plt.scatter(X_test, y_pred_test, color='red', label='Predicted values')\n",
        "plt.xlabel(\"Feature X\")\n",
        "plt.ylabel(\"Target y\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to\n",
        "optimise the performance of the model. Use grid search or random search to find the best\n",
        "hyperparameters\n",
        "\"\"\"\n",
        "## GridSearch\n",
        "# Grid search tests all possible combinations of hyperparameters within the given range.\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#from sklearn.svm import SVC # This import is not used\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Define the hyperparameters grid\n",
        "# Removed 'learning_rate' as it's not a valid parameter for RandomForestClassifier\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],    # Number of trees\n",
        "    'max_depth': [None, 10, 20, 30],    # Max depth of each tree\n",
        "    'min_samples_split': [2, 5, 10]     # Minimum number of samples required to split an internal node\n",
        "}\n",
        "\n",
        "# Setup grid search with cross-validation\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Fit grid search\n",
        "grid_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters found: \", grid_search.best_params_)\n",
        "\n",
        "# Best model\n",
        "best_model = grid_search.best_estimator_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOqwd9gdDNxS",
        "outputId": "8fc879e2-e8ce-4859-8b1f-e674ccf7ce0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
            "Best parameters found:  {'max_depth': None, 'min_samples_split': 2, 'n_estimators': 100}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Random search picks random combinations of hyperparameters from the given ranges,\n",
        "# which can sometimes be more efficient if there are many parameters to tune\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.datasets import load_iris\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Load data\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Define the model\n",
        "model = RandomForestClassifier()\n",
        "\n",
        "# Define the hyperparameters distribution\n",
        "param_dist = {\n",
        "    'n_estimators': randint(50, 200),    # Random number of trees between 50 and 200\n",
        "    'max_depth': [None, 10, 20, 30],      # Max depth choices\n",
        "    'min_samples_split': randint(2, 10),  # Random min samples split between 2 and 10\n",
        "}\n",
        "\n",
        "# Setup random search with cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=100, cv=5, n_jobs=-1, random_state=42)\n",
        "\n",
        "# Fit random search\n",
        "random_search.fit(X, y)\n",
        "\n",
        "# Print the best parameters found\n",
        "print(\"Best parameters found: \", random_search.best_params_)\n",
        "\n",
        "# Best model\n",
        "best_model = random_search.best_estimator_\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNfXP6cQGhTI",
        "outputId": "d3832203-5faa-486b-9e4c-bfea3376c9ce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters found:  {'max_depth': 20, 'min_samples_split': 5, 'n_estimators': 142}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sMoLoYnZGvsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. What is a weak learner in Gradient Boosting?\n",
        "Ans: \\\n",
        "In **Gradient Boosting**, a **weak learner** refers to a model that performs slightly better than random guessing. In the context of gradient boosting algorithms, weak learners are typically **decision trees** with limited depth, such as **stumps** (trees with just one split).\n",
        "\n",
        "### Why are weak learners used in Gradient Boosting?\n",
        "\n",
        "The idea behind using weak learners is that, instead of relying on a single strong model, **gradient boosting** builds a sequence of models where each new model tries to correct the mistakes (residuals) made by the previous models. This sequential learning approach allows the ensemble to gradually improve its performance.\n",
        "\n",
        "- **Weak learners** are intentionally simple models, usually decision trees with shallow depth (often just a **decision stump**), because they are fast to train and easy to modify to focus on the mistakes made by previous models.\n",
        "- Each weak learner, when added to the ensemble, attempts to minimize the residual errors from the combined predictions of all previous models.\n",
        "\n",
        "### The Gradient Boosting Process:\n",
        "\n",
        "1. **Initialization**: The process begins by initializing the model with a simple base model (e.g., predicting the mean for regression or the most frequent class for classification).\n",
        "  \n",
        "2. **Training weak learners**: After each iteration, a weak learner (typically a shallow decision tree) is trained to predict the residual errors from the previous model's predictions.\n",
        "  \n",
        "3. **Model update**: The predictions of the weak learner are then added to the model, and the weights of the trees are updated based on the errors (or gradients) of the current model.\n",
        "\n",
        "4. **Repeat**: This process repeats for multiple iterations, with each weak learner correcting the errors made by the ensemble model up to that point.\n",
        "\n",
        "### Why weak learners?\n",
        "- **Simplicity**: By using weak learners, the model can focus on learning from the errors made by previous iterations. Complex models can overfit to the data and might not generalize well.\n",
        "- **Boosting**: The sequential addition of weak learners, each focusing on improving the performance of the entire model, leads to a highly effective ensemble, which is strong in performance but still built from simple, weak learners.\n",
        "\n",
        "### Key Point:\n",
        "In gradient boosting, weak learners (like shallow decision trees) are not necessarily \"bad\" models; they are just models that alone wouldn't perform well, but when combined with other models in a boosting framework, they form a much stronger predictive model.\n",
        "\n",
        "Would you like more details on how the weak learner contributes to the overall boosting algorithm, or are you working on applying this concept in a specific project?"
      ],
      "metadata": {
        "id": "JBB7g-qKHMBm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ux0qva6JHeoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
        "Ans: \\\n",
        "The step-by-step intuition:\n",
        "\n",
        "### 1. **Ensemble Learning**\n",
        "- **Gradient Boosting** is an **ensemble method**, meaning it combines multiple weak models to create a strong model.\n",
        "- The goal is to minimize a **loss function** (like Mean Squared Error for regression or Log Loss for classification) by adding models that progressively correct the errors made by previous ones.\n",
        "\n",
        "### 2. **Weak Learners** and Iterative Learning\n",
        "- **Weak Learners**: These are simple models (typically shallow decision trees) that perform just slightly better than random guessing.\n",
        "- The idea is not to build a single, complex model but rather a series of simple models that work together to make accurate predictions.\n",
        "\n",
        "### 3. **Gradient Descent Optimization**\n",
        "- The term **\"gradient\"** in Gradient Boosting comes from the fact that the algorithm uses **gradient descent** to minimize the loss function.\n",
        "- **Gradient descent** is an optimization technique used to find the minimum of a function by moving in the direction of the steepest decrease (i.e., the negative gradient).\n",
        "- In Gradient Boosting, each new weak learner is trained to predict the residuals (errors) of the previous model, effectively following the negative gradient of the loss function.\n",
        "\n",
        "### 4. **Boosting Process (Sequential Addition of Learners)**\n",
        "- Start with an initial model: The initial model can be as simple as predicting the mean (for regression) or the most frequent class (for classification).\n",
        "  \n",
        "- **Fit a weak learner**: In the next step, train a weak learner (a small decision tree) to predict the **residual errors** (the difference between the current model’s predictions and the true target values).\n",
        "  \n",
        "- **Update the model**: Add the predictions from this new learner to the existing model, adjusting the model's output in a way that reduces the residual errors.\n",
        "  \n",
        "- **Iterate**: Repeat the process for several iterations, where each new weak learner is trained on the residual errors of the combined previous models. Each learner adds more corrective power to the overall model.\n",
        "\n",
        "### 5. **Learning Rate (Shrinkage)**\n",
        "- **Learning rate** controls how much each weak learner contributes to the final model. A smaller learning rate means that each weak learner's influence is reduced, and the model requires more iterations to converge.\n",
        "- By choosing a small learning rate and increasing the number of learners, we can prevent overfitting and improve the generalization of the model.\n",
        "\n",
        "### 6. **Why it works:**\n",
        "- **Residuals or Errors**: Each weak learner in the sequence focuses on the residuals or errors made by the model so far. It adjusts the current predictions to improve the overall model performance.\n",
        "- **Gradient Descent**: The algorithm minimizes the loss function (a measure of how wrong the model's predictions are) by following the gradient (direction of steepest decrease). This ensures that the sequence of learners is always moving towards improving the model’s accuracy.\n",
        "\n",
        "### 7. **Key Properties:**\n",
        "- **Sequential Learning**: The model builds iteratively, learning from previous mistakes.\n",
        "- **Additive Model**: The model is built by adding learners one by one, with each new learner trying to reduce the residual error.\n",
        "- **Overfitting Control**: With hyperparameters like **learning rate**, **maximum depth of trees**, and **number of trees**, gradient boosting can be controlled to avoid overfitting.\n",
        "- **Flexibility**: Gradient boosting can be applied to various loss functions (like regression, classification, etc.) and uses decision trees as the base learners.\n",
        "\n",
        "### Example with Intuition:\n",
        "\n",
        "Let's say we are predicting house prices (a regression problem):\n",
        "1. Start with an initial model that predicts the **mean house price**. This is your base prediction.\n",
        "2. The first weak learner (a decision tree) tries to predict the **errors** (the difference between the actual prices and the predicted mean).\n",
        "3. Add this new learner's prediction to the base prediction. Now, the model is a combination of the initial mean prediction and the correction from the first tree.\n",
        "4. The next weak learner focuses on correcting the residual errors from the combined model (which is the difference between actual prices and the current combined predictions).\n",
        "5. Repeat this process until a stopping criterion is met, such as a maximum number of iterations or when the improvement in the model becomes negligible.\n"
      ],
      "metadata": {
        "id": "XU8-tjEmHfFi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
        "Ans: \\\n",
        "\n",
        "###  **Core Idea**:\n",
        "Gradient Boosting builds an ensemble **sequentially**, where **each weak learner tries to correct the errors** made by the combined previous learners.\n",
        "\n",
        "---\n",
        "\n",
        "###  Step-by-Step Breakdown:\n",
        "\n",
        "#### 1. **Initialize the Model**\n",
        "- Start with a simple prediction for all data points.  \n",
        "  - For regression: use the **mean** of the target values.  \n",
        "  - For classification: use **log odds** or **class probabilities**.\n",
        "\n",
        "Let’s denote this as:\n",
        "```\n",
        "F₀(x) = initial prediction\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Iterate to Add Weak Learners**\n",
        "For `m = 1 to M` (where `M` is the number of trees):\n",
        "\n",
        "##### a. **Compute the Residuals (or Gradients)**\n",
        "- These are the negative gradients of the loss function with respect to the current model predictions.\n",
        "- Think of them as the **errors** the current model is making.\n",
        "\n",
        "For example, in regression with MSE loss:\n",
        "```\n",
        "residual = yᵢ - Fₘ₋₁(xᵢ)\n",
        "```\n",
        "\n",
        "In classification (e.g., using Log Loss), the gradient is more complex, but it still represents the direction to reduce the error.\n",
        "\n",
        "---\n",
        "\n",
        "##### b. **Train a Weak Learner on the Residuals**\n",
        "- Fit a new decision tree (usually a **shallow tree**, like depth 3) to predict the residuals.\n",
        "```\n",
        "hₘ(x) ≈ residuals\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "##### c. **Update the Model**\n",
        "- Add the predictions of the new tree to the existing model, scaled by a **learning rate (α)**:\n",
        "```\n",
        "Fₘ(x) = Fₘ₋₁(x) + α * hₘ(x)\n",
        "```\n",
        "\n",
        "- This way, each tree is **correcting** the mistakes made by the current model.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Final Model**\n",
        "After `M` iterations (i.e., adding `M` trees), you get a strong ensemble model:\n",
        "```\n",
        "F_M(x) = F₀(x) + α * h₁(x) + α * h₂(x) + ... + α * h_M(x)\n",
        "```\n",
        "\n",
        "Each `hᵢ(x)` is a **weak learner**, but together they form a **powerful ensemble**.\n",
        "\n",
        "---\n",
        "\n",
        "### 🔧 Key Components:\n",
        "- **Weak Learners**: Usually shallow decision trees.\n",
        "- **Loss Function**: Guides the boosting process (e.g., MSE for regression, Log Loss for classification).\n",
        "- **Gradient**: Residuals are based on the derivative (gradient) of the loss function.\n",
        "- **Learning Rate (α)**: Controls how much each tree contributes (smaller values slow learning and reduce overfitting).\n",
        "- **Number of Trees (M)**: More trees can improve accuracy but also risk overfitting."
      ],
      "metadata": {
        "id": "2C-IfgYkINxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize Gradient Boosting Classifier\n",
        "gb_clf = GradientBoostingClassifier(\n",
        "    n_estimators=100,      # Number of trees (weak learners)\n",
        "    learning_rate=0.1,     # Shrinkage factor for each tree's contribution\n",
        "    max_depth=3,           # Depth of each weak learner (shallow tree)\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "gb_clf.fit(X_train, y_train)\n",
        "\n",
        "# Predict on test set\n",
        "y_pred = gb_clf.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azPaZinHILG9",
        "outputId": "8edac353-53b4-47be-f446-3e8a3951fb0d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  What’s Happening Under the Hood:\n",
        "- `n_estimators=100`: The model will add **100 weak learners** (small decision trees).\n",
        "- `learning_rate=0.1`: Each tree's prediction is **scaled** to prevent overfitting.\n",
        "- `max_depth=3`: Each weak learner is a tree of depth 3 (not a full decision tree).\n",
        "\n",
        "Each iteration:\n",
        "1. Computes the residual errors from the current model.\n",
        "2. Fits a new decision tree on those residuals.\n",
        "3. Adds the new tree to the existing model, slightly adjusting the final prediction."
      ],
      "metadata": {
        "id": "iwZNoioWJD3L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
        "Ans: \\\n",
        "\n",
        "##  **Steps to Construct the Mathematical Intuition of Gradient Boosting**\n",
        "\n",
        "###  Problem Setup:\n",
        "We are given a dataset:\n",
        "- Inputs: $( x_1, x_2, \\dots, x_n \\in \\mathbb{R}^d $)\n",
        "- Targets: $( y_1, y_2, \\dots, y_n \\in \\mathbb{R} )$ (for regression) or class labels (for classification)\n",
        "\n",
        "The goal is to **learn a function** \\( F(x) \\) that **minimizes a loss function** $( \\mathcal{L}(y, F(x)) )$.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Step 1: Define the Loss Function**\n",
        "Choose an appropriate loss function for your task:\n",
        "- Regression → Mean Squared Error (MSE):  \n",
        "  $$[ \\mathcal{L}(y, F(x)) = \\frac{1}{2}(y - F(x))^2 ]$$\n",
        "- Classification → Log Loss:  \n",
        "  $$[ \\mathcal{L}(y, F(x)) = -[y \\log(\\hat{p}) + (1-y)\\log(1 - \\hat{p})] ]$$\n",
        "\n",
        "This is the function we aim to **minimize**.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Step 2: Initialize the Model**\n",
        "Start with a simple prediction that minimizes the loss over all training data.\n",
        "\n",
        "For MSE, it's:\n",
        "$$[\n",
        "F_0(x) = \\arg\\min_c \\sum_{i=1}^{n} \\mathcal{L}(y_i, c) = \\text{mean}(y)\n",
        "]$$\n",
        "\n",
        "---\n",
        "\n",
        "###  **Step 3: Iteratively Add Weak Learners**\n",
        "\n",
        "Repeat for $( m = 1 ) to ( M )$ (number of boosting rounds):\n",
        "\n",
        "####  3a. Compute Negative Gradients\n",
        "These are the **residuals** or **pseudo-residuals**, i.e., the direction in which the current model's predictions should be adjusted to reduce loss.\n",
        "\n",
        "For each data point:\n",
        "$$[\n",
        "r_i^{(m)} = -\\left[ \\frac{\\partial \\mathcal{L}(y_i, F(x_i))}{\\partial F(x_i)} \\right]_{F(x) = F_{m-1}(x)}\n",
        "]$$\n",
        "\n",
        "> This is the **core idea**: the gradient of the loss function tells us how to adjust the model to improve performance.\n",
        "\n",
        "---\n",
        "\n",
        "####  3b. Fit a Weak Learner to the Residuals\n",
        "Train a weak learner $( h_m(x) )$ (like a shallow decision tree) to fit the residuals $( r_i^{(m)} )$.\n",
        "\n",
        "$$[\n",
        "h_m(x) \\approx r_i^{(m)}\n",
        "]$$\n",
        "\n",
        "---\n",
        "\n",
        "####  3c. Compute Multiplier (Optional Step)\n",
        "Sometimes a **line search** is used to find the best multiplier \\( \\gamma_m \\) that minimizes loss:\n",
        "\n",
        "$$[\n",
        "\\gamma_m = \\arg\\min_\\gamma \\sum_{i=1}^{n} \\mathcal{L}\\left(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i)\\right)\n",
        "]$$\n",
        "\n",
        "---\n",
        "\n",
        "####  3d. Update the Model\n",
        "Add the new learner to the current model (scaled by the learning rate $( \\alpha ))$:\n",
        "\n",
        "$$[\n",
        "F_m(x) = F_{m-1}(x) + \\alpha \\cdot \\gamma_m \\cdot h_m(x)\n",
        "]$$\n",
        "\n",
        "This gradually improves the model by **correcting previous mistakes**.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Step 4: Final Model**\n",
        "After \\( M \\) iterations:\n",
        "$$[\n",
        "F_M(x) = F_0(x) + \\alpha \\sum_{m=1}^{M} \\gamma_m h_m(x)\n",
        "]$$\n",
        "\n",
        "This is your final boosted model — a sum of **weak learners**, each nudging the prediction in the right direction using gradient information.\n",
        "\n",
        "---\n",
        "\n",
        "###  Example: MSE Loss (Regression)\n",
        "Let’s simplify the math for Mean Squared Error:\n",
        "\n",
        "- Loss: $( \\mathcal{L}(y, F(x)) = \\frac{1}{2}(y - F(x))^2 )$$\n",
        "- Gradient:  \n",
        "  $$[\n",
        "  r_i^{(m)} = -\\frac{\\partial \\mathcal{L}}{\\partial F(x)} = y_i - F_{m-1}(x_i)\n",
        "  ]$$\n",
        "\n",
        "So each weak learner is just trained to predict the **residuals**!\n",
        "\n",
        "---\n",
        "\n",
        "###  Summary Table\n",
        "\n",
        "| Step | What Happens | Math |\n",
        "|------|--------------|------|\n",
        "| 1. Initialize | Predict a constant | \\( F_0(x) = \\arg\\min_c \\sum \\mathcal{L}(y_i, c) \\) |\n",
        "| 2. Compute Gradient | Residuals of loss | \\( r_i = -\\frac{\\partial \\mathcal{L}}{\\partial F(x)} \\) |\n",
        "| 3. Fit Learner | Fit tree on residuals | \\( h_m(x) \\approx r_i \\) |\n",
        "| 4. Update | Add tree to model | \\( F_m(x) = F_{m-1}(x) + \\alpha h_m(x) \\) |\n"
      ],
      "metadata": {
        "id": "RWvVd5iBJQoE"
      }
    }
  ]
}