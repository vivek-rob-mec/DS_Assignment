{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Q1. What is hierarchical clustering, and how is it different from other clustering techniques?**\n",
        "Ans: \\\n",
        "\n",
        "**Hierarchical clustering** builds a hierarchy of clusters either by:\n",
        "- **Merging smaller clusters** into larger ones (bottom-up / agglomerative), or\n",
        "- **Dividing a big cluster** into smaller ones (top-down / divisive).\n",
        "\n",
        "**Key Differences from Other Techniques:**\n",
        "- **Does not require pre-defining the number of clusters (unlike K-Means)**.\n",
        "- Produces a **dendrogram** (tree-like structure) to visualize the merging/splitting.\n",
        "- Can handle **nested clusters** better than flat clustering algorithms.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q2. What are the two main types of hierarchical clustering algorithms? Describe each in brief.**\n",
        "Ans: \\\n",
        "\n",
        "1. **Agglomerative Hierarchical Clustering (Bottom-Up)**\n",
        "   - Start with each point as its own cluster.\n",
        "   - Merge the two **closest clusters** step-by-step.\n",
        "   - Stop when all points are merged into one large cluster or based on a threshold.\n",
        "\n",
        "2. **Divisive Hierarchical Clustering (Top-Down)**\n",
        "   - Start with one large cluster (all points).\n",
        "   - Recursively **split** clusters until each point is its own cluster or a desired number is reached.\n",
        "   - Less common than agglomerative due to higher computational cost.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q3. How do you determine the distance between two clusters in hierarchical clustering, and what are the common distance metrics used?**\n",
        "Ans: \\\n",
        "\n",
        "The way you measure distance between **clusters** is called a **linkage criterion**:\n",
        "\n",
        "**Common Linkage Methods:**\n",
        "- **Single linkage**: Minimum distance between any two points from each cluster.\n",
        "- **Complete linkage**: Maximum distance between any two points.\n",
        "- **Average linkage**: Average distance between all pairs of points.\n",
        "- **Ward’s linkage**: Minimizes the increase in total within-cluster variance.\n",
        "\n",
        "**Common Distance Metrics:**\n",
        "- **Euclidean** (most common for numerical data)\n",
        "- **Manhattan**\n",
        "- **Cosine similarity** (for text data)\n",
        "- **Hamming distance** (for categorical data)\n",
        "\n",
        "---\n",
        "\n",
        "### **Q4. How do you determine the optimal number of clusters in hierarchical clustering, and what are some common methods used for this purpose?**\n",
        "Ans: \\\n",
        "\n",
        "Common methods include:\n",
        "\n",
        "1. **Dendrogram Cut-Off**  \n",
        "   - Look for the **longest vertical line** (biggest jump in distance) in the dendrogram.\n",
        "   - Draw a horizontal line to cut the tree — the number of intersections = optimal clusters.\n",
        "\n",
        "2. **Silhouette Score**  \n",
        "   - Evaluates how well-separated the clusters are.\n",
        "   - Values range from -1 to 1 — higher is better.\n",
        "\n",
        "3. **Elbow Method (on linkage distances)**  \n",
        "   - Plot number of clusters vs. linkage distance.\n",
        "   - Look for the point where the drop in distance slows down.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q5. What are dendrograms in hierarchical clustering, and how are they useful in analyzing the results?**\n",
        "Ans: \\\n",
        "\n",
        "A **dendrogram** is a **tree-like diagram** that shows how clusters are formed by successively merging or splitting points.\n",
        "\n",
        "**How it's useful:**\n",
        "- Helps visualize the **structure and hierarchy** of clusters.\n",
        "- You can **cut the dendrogram** at different levels to choose how many clusters to form.\n",
        "- Reveals **natural groupings** and **outliers**.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q6. Can hierarchical clustering be used for both numerical and categorical data? If yes, how are the distance metrics different for each type of data?**\n",
        "Ans: \\\n",
        "\n",
        "**Yes**, hierarchical clustering can handle both:\n",
        "\n",
        "- **Numerical data**: Use **Euclidean**, **Manhattan**, or **Ward's method**.\n",
        "- **Categorical data**: Use **Hamming distance**, **Jaccard index**, or **Gower distance**.\n",
        "- **Mixed data** (numerical + categorical): Use **Gower distance**, which handles both types.\n",
        "\n",
        "Libraries like `scipy`, `sklearn`, and `gower` in Python can help compute appropriate distance matrices.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q7. How can you use hierarchical clustering to identify outliers or anomalies in your data?**\n",
        "Ans: \\\n",
        "\n",
        "**Outliers** can be identified by:\n",
        "- Looking at **small clusters** that are far from the rest in the dendrogram.\n",
        "- Observing **data points that merge late** (at a large distance threshold).\n",
        "- Plotting the dendrogram and checking for **single-point merges** (these could be anomalies).\n",
        "\n",
        "**Tip:** Outliers tend to **remain isolated** until the very end of the merging process."
      ],
      "metadata": {
        "id": "YLA47fbXZF1P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q5ualw4UZnGC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}