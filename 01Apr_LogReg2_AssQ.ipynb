{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. What is the purpose of grid search cv in machine learning, and how does it work?\n",
        "Ans: \\\n",
        "\n",
        "###  **Purpose of Grid Search CV:**\n",
        "\n",
        "**Grid Search with Cross-Validation (GridSearchCV)** is used to:\n",
        "\n",
        "- **Find the best combination of hyperparameters** for a machine learning model.\n",
        "- **Improve model performance** by systematically testing multiple configurations.\n",
        "- **Avoid overfitting or underfitting** by evaluating model performance using **cross-validation**.\n",
        "\n",
        " **Hyperparameters** are settings that you manually define **before** training (e.g., `C`, `penalty`, `max_depth`, `learning_rate`, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "###  **How Grid Search CV Works:**\n",
        "\n",
        "1. **Define a grid of hyperparameters**  \n",
        "   Example for logistic regression:\n",
        "   ```python\n",
        "   param_grid = {\n",
        "       'C': [0.01, 0.1, 1, 10],\n",
        "       'penalty': ['l1', 'l2']\n",
        "   }\n",
        "   ```\n",
        "\n",
        "2. **Choose a scoring metric** (e.g., accuracy, F1-score, AUC)\n",
        "\n",
        "3. **Split the data using cross-validation (CV)**  \n",
        "   - For example, **5-fold CV** splits the data into 5 parts, trains on 4, tests on 1, and rotates.\n",
        "\n",
        "4. **Train the model for every combination** of hyperparameters on each fold.\n",
        "\n",
        "5. **Evaluate and record the performance** for each combination.\n",
        "\n",
        "6. **Select the best set of hyperparameters** based on the **average CV score**.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Example with Scikit-Learn:**\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(solver='liblinear')\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "print(\"Best score:\", grid_search.best_score_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "###  **Benefits of Grid Search CV:**\n",
        "\n",
        "| Benefit                         | Explanation |\n",
        "|----------------------------------|-------------|\n",
        "|  Improves performance          | Finds the best configuration for the model |\n",
        "|  Reduces overfitting risk      | Uses cross-validation to test robustness |\n",
        "|  Systematic and exhaustive     | Tries all possible combinations |\n",
        "\n",
        "---\n",
        "\n",
        "###  **Limitations:**\n",
        "\n",
        "- **Computationally expensive** if the grid is large\n",
        "- Might miss optimal values if they‚Äôre not in the grid  \n",
        "  üîß *Solution: Use **RandomizedSearchCV** or **Bayesian Optimization** for large spaces.*"
      ],
      "metadata": {
        "id": "uxEH0NlB6qEC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. Describe the difference between grid search cv and randomize search cv, and when might you choose one over the other?\n",
        "Ans: \\\n",
        "\n",
        "###  **1. Grid Search CV**\n",
        "\n",
        "**How it works:**\n",
        "- Tries **every possible combination** of hyperparameters from the defined grid.\n",
        "- Exhaustive and systematic.\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "```\n",
        "- Total combinations: 4 (C) √ó 2 (penalty) = 8 models tested\n",
        "\n",
        "**Pros:**\n",
        " Finds the best result **within the grid**  \n",
        " Works well for **small grids** or **limited parameters**\n",
        "\n",
        "**Cons:**\n",
        " **Time-consuming** and **computationally expensive**  \n",
        " Might miss better values **outside the grid**\n",
        "\n",
        "---\n",
        "\n",
        "###  **2. Randomized Search CV**\n",
        "\n",
        "**How it works:**\n",
        "- Randomly selects **a fixed number of combinations** from the parameter space.\n",
        "- You specify how many iterations to try (`n_iter`).\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "param_dist = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2']\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(model, param_distributions=param_dist, n_iter=5, cv=5)\n",
        "```\n",
        "\n",
        "**Pros:**\n",
        " **Much faster**, especially with **large hyperparameter spaces**  \n",
        " Can explore **more diverse combinations**  \n",
        " Can use **distributions** for continuous hyperparameters\n",
        "\n",
        "**Cons:**\n",
        " Doesn‚Äôt guarantee the absolute best combination  \n",
        " Results may vary between runs (but can be fixed using `random_state`)\n",
        "\n",
        "---\n",
        "\n",
        "###  Summary:\n",
        "\n",
        "| Feature              | Grid Search CV       | Randomized Search CV     |\n",
        "|----------------------|----------------------|---------------------------|\n",
        "| Search Type          | Exhaustive           | Random Sampling           |\n",
        "| Speed                | Slower               | Faster                    |\n",
        "| Accuracy             | Higher (within grid) | Slightly lower (trade-off)|\n",
        "| Best Use Case        | Small, specific grids| Large, broad search spaces|\n"
      ],
      "metadata": {
        "id": "n1KLz0CP6qg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. What is data leakage, and why is it a problem in machine learning? Provide an example.\n",
        "Ans: \\\n",
        "\n",
        "###  **Definition:**\n",
        "\n",
        "**Data leakage** occurs when **information from outside the training dataset** ‚Äî usually from the **test set or future data** ‚Äî is used to create the model.  \n",
        "This gives the model **unfair or unrealistic access** to data it **wouldn‚Äôt have at prediction time**.\n",
        "\n",
        " It leads to **overly optimistic performance** during training/testing ‚Äî but **poor real-world performance**.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Why is Data Leakage a Problem?**\n",
        "\n",
        "- The model **learns from information it shouldn‚Äôt have**.\n",
        "- Performance looks great in validation, but **fails in production**.\n",
        "- It **violates the principle of generalization**.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Types of Data Leakage:**\n",
        "\n",
        "#### 1. **Train-Test Contamination**\n",
        "- Training data ‚Äúleaks‚Äù into test data (or vice versa).\n",
        "- Example: Normalizing the entire dataset **before** splitting.\n",
        "\n",
        "#### 2. **Feature Leakage**\n",
        "- Features include **data that wouldn't be known at prediction time**.\n",
        "- Example: A feature contains information derived from the **target variable**.\n",
        "\n",
        "---\n",
        "\n",
        "###  **Example of Data Leakage:**\n",
        "\n",
        "**Scenario: Predicting if a customer will default on a loan**\n",
        "\n",
        "| Feature               | Target (Default) |\n",
        "|-----------------------|------------------|\n",
        "| Income                | 1 (Yes)          |\n",
        "| Number of missed payments in next 6 months |  (Leak!) |\n",
        "| Credit score          | 0 (No)           |\n",
        "\n",
        " *Problem:*  \n",
        "‚ÄúNumber of missed payments in next 6 months‚Äù is **known only in the future** ‚Äî it directly reveals the target, so the model will learn to cheat.\n",
        "\n",
        " *Fix:* Use only features that would be known **at the time of decision**, like income, current debt, credit history, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### **How to Prevent Data Leakage:**\n",
        "\n",
        "| Practice                        | Description |\n",
        "|----------------------------------|-------------|\n",
        "| Split data early             | Split into train/test **before** doing preprocessing |\n",
        "| Preprocess using pipelines   | Use tools like `sklearn` pipelines to avoid leakage |\n",
        "| Avoid target-derived features | Don‚Äôt use any feature that leaks the target outcome |\n",
        "| Inspect feature correlations | Features with unusually high correlation to target may be leaking |\n"
      ],
      "metadata": {
        "id": "7jat10rs6qvm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. How can you prevent data leakage when building a machine learning model?\n",
        "Ans: \\\n",
        "**Data Leakage** occurs when **information from outside the training dataset** (like test data or future data) is used in the model, leading to overly optimistic results during training but poor real-world performance.\n",
        "\n",
        "### **How to Prevent Data Leakage:**\n",
        "\n",
        "1. **Split data early**: Always split the dataset into training and test sets **before** any preprocessing.\n",
        "2. **Use pipelines**: Ensure preprocessing steps are only applied to training data, not the entire dataset.\n",
        "3. **Avoid future data**: Don‚Äôt use features that wouldn‚Äôt be available at prediction time (e.g., future values).\n",
        "4. **Don‚Äôt use target variables as features**: Never include the target variable (what you‚Äôre predicting) in the feature set.\n",
        "5. **Be careful with time-series data**: Use chronological splits to prevent using future data in the training set.\n",
        "6. **Cross-validation**: Ensure proper separation of training and test sets during cross-validation.\n",
        "\n",
        "These steps help ensure your model is **fair** and its performance reflects how it will behave in real-world predictions."
      ],
      "metadata": {
        "id": "-1iKB5iQ6rGu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?\n",
        "Ans: \\\n",
        "\n",
        "A **confusion matrix** is a table used to **evaluate the performance** of a classification model. It shows how many predictions were:\n",
        "\n",
        "- **Correct** (True Positives & True Negatives)\n",
        "- **Incorrect** (False Positives & False Negatives)\n",
        "\n",
        "---\n",
        "\n",
        "###  **Structure of a Confusion Matrix (for binary classification):**\n",
        "\n",
        "|                  | **Predicted Positive** | **Predicted Negative** |\n",
        "|------------------|------------------------|------------------------|\n",
        "| **Actual Positive** | True Positive (TP)       | False Negative (FN)      |\n",
        "| **Actual Negative** | False Positive (FP)      | True Negative (TN)       |\n",
        "\n",
        "---\n",
        "\n",
        "###  **What It Tells You:**\n",
        "\n",
        "- **TP (True Positive):** Correctly predicted positive cases  \n",
        "- **TN (True Negative):** Correctly predicted negative cases  \n",
        "- **FP (False Positive):** Incorrectly predicted positive (Type I error)  \n",
        "- **FN (False Negative):** Incorrectly predicted negative (Type II error)\n",
        "\n",
        "---\n",
        "\n",
        "###  **Metrics Derived from the Confusion Matrix:**\n",
        "\n",
        "| Metric             | Formula                                      | What it Tells You                          |\n",
        "|--------------------|----------------------------------------------|---------------------------------------------|\n",
        "| **Accuracy**       | (TP + TN) / (TP + TN + FP + FN)              | Overall correctness                         |\n",
        "| **Precision**      | TP / (TP + FP)                               | How many predicted positives are correct    |\n",
        "| **Recall (Sensitivity)** | TP / (TP + FN)                        | How many actual positives were captured     |\n",
        "| **F1-Score**       | 2 √ó (Precision √ó Recall) / (Precision + Recall) | Balance between precision and recall        |\n",
        "\n",
        "---\n",
        "\n",
        "###  **Why It‚Äôs Useful:**\n",
        "\n",
        "- Helps identify **where the model is making errors**\n",
        "- Useful for **imbalanced datasets** (accuracy alone can be misleading)\n",
        "- Helps choose the right metric (precision vs recall)"
      ],
      "metadata": {
        "id": "bOhu5ViR6rXN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6. Explain the difference between precision and recall in the context of a confusion matrix.\n",
        "Ans: \\\n",
        "\n",
        "###  Confusion Matrix Recap:\n",
        "\n",
        "|                           | **Predicted Positive** | **Predicted Negative** |\n",
        "|---------------------------|------------------------|------------------------|\n",
        "| **Actual Positive**       |  **True Positive (TP)** |  **False Negative (FN)** |\n",
        "| **Actual Negative**       |  **False Positive (FP)** |  **True Negative (TN)** |\n",
        "\n",
        "---\n",
        "\n",
        "###  **Precision**  \n",
        "**Definition:** Out of all the predicted **positives**, how many were **actually** positive?\n",
        "\n",
        "**Formula:**  \n",
        "$$[\n",
        "\\text{Precision} = \\frac{TP}{TP + FP}\n",
        "]$$\n",
        "\n",
        "**Focus:** Quality of positive predictions  \n",
        "**High Precision = Few False Positives**\n",
        "\n",
        " **Use case:** When **false positives** are costly (e.g., spam detection ‚Äî don't block legit emails)\n",
        "\n",
        "---\n",
        "\n",
        "###  **Recall (Sensitivity or True Positive Rate)**  \n",
        "**Definition:** Out of all **actual positives**, how many were **correctly predicted**?\n",
        "\n",
        "**Formula:**  \n",
        "$$[\n",
        "\\text{Recall} = \\frac{TP}{TP + FN}\n",
        "]$$\n",
        "\n",
        "**Focus:** Capturing all actual positives  \n",
        "**High Recall = Few False Negatives**\n",
        "\n",
        " **Use case:** When **missing a positive case** is costly (e.g., disease detection ‚Äî don't miss a sick patient)\n",
        "\n",
        "---\n",
        "\n",
        "###  **Summary Table:**\n",
        "\n",
        "| Metric     | Measures                      | Focus               | Key Concern       |\n",
        "|------------|-------------------------------|----------------------|-------------------|\n",
        "| Precision  | TP / (TP + FP)                | Accuracy of positives | False Positives   |\n",
        "| Recall     | TP / (TP + FN)                | Coverage of positives | False Negatives   |\n",
        "\n",
        "---\n",
        "\n",
        "###  **Balance:**  \n",
        "- Use **F1-score** to balance both when **you need a trade-off** between precision and recall."
      ],
      "metadata": {
        "id": "HhgBoi3L6rtc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?\n",
        "Ans: \\\n",
        "\n",
        "A **confusion matrix** tells you not just how often your model is right‚Äîbut **how** it‚Äôs wrong.\n",
        "\n",
        "###  Confusion Matrix Breakdown:\n",
        "\n",
        "|                           | **Predicted Positive** | **Predicted Negative** |\n",
        "|---------------------------|------------------------|------------------------|\n",
        "| **Actual Positive**       |  **True Positive (TP)** |  **False Negative (FN)** |\n",
        "| **Actual Negative**       |  **False Positive (FP)** |  **True Negative (TN)** |\n",
        "\n",
        "---\n",
        "\n",
        "###  **Step-by-Step Interpretation:**\n",
        "\n",
        "1. **Look at False Positives (FP):**\n",
        "   - These are cases **predicted as positive** but actually **negative**.\n",
        "   - **Impact:** The model is too \"eager\" to say something is positive.\n",
        "   - **Example:** Predicting a non-spam email as spam.\n",
        "\n",
        "2. **Look at False Negatives (FN):**\n",
        "   - These are cases **predicted as negative** but actually **positive**.\n",
        "   - **Impact:** The model is **missing real positive cases**.\n",
        "   - **Example:** Missing a cancer diagnosis (bad in medical scenarios).\n",
        "\n",
        "3. **Compare FP and FN counts:**\n",
        "   - **More FP?** Model needs to be more precise.\n",
        "   - **More FN?** Model needs better recall.\n",
        "\n",
        "4. **Evaluate with context:**\n",
        "   - What‚Äôs worse in your application: **a false alarm** or **missing a real case**?\n",
        "\n",
        "---\n",
        "\n",
        "###  Example:\n",
        "\n",
        "Imagine you built a model to detect fraudulent transactions:\n",
        "\n",
        "|                           | Predicted Fraud | Predicted Safe |\n",
        "|---------------------------|-----------------|----------------|\n",
        "| **Actual Fraud**          | TP = 80         | FN = 20        |\n",
        "| **Actual Safe**           | FP = 40         | TN = 860       |\n",
        "\n",
        "- **FN = 20** ‚Üí 20 frauds went undetected   \n",
        "- **FP = 40** ‚Üí 40 safe transactions were wrongly flagged   \n",
        "\n",
        "**Interpretation:**  \n",
        "- If **missing fraud** is worse, work on improving **recall** (reduce FN).  \n",
        "- If **bothering users too often** is a problem, improve **precision** (reduce FP).\n",
        "\n",
        "---\n",
        "\n",
        "###  Summary:\n",
        "\n",
        "- **FP** ‚Üí Model says ‚Äúyes‚Äù when it should say ‚Äúno‚Äù (false alarm).\n",
        "- **FN** ‚Üí Model says ‚Äúno‚Äù when it should say ‚Äúyes‚Äù (missed case).\n",
        "- **Interpreting both** helps you tweak your model to focus on what matters more for your use case."
      ],
      "metadata": {
        "id": "k1aYmING6sAb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?\n",
        "Ans: \\\n",
        "\n",
        "Given the **confusion matrix**:\n",
        "\n",
        "|                           | **Predicted Positive** | **Predicted Negative** |\n",
        "|---------------------------|------------------------|------------------------|\n",
        "| **Actual Positive**       | **TP (True Positive)** | **FN (False Negative)** |\n",
        "| **Actual Negative**       | **FP (False Positive)** | **TN (True Negative)** |\n",
        "\n",
        "---\n",
        "\n",
        "###  **Key Metrics:**\n",
        "\n",
        "| **Metric**       | **Formula**                                  | **What it Tells You**                              |\n",
        "|------------------|-----------------------------------------------|----------------------------------------------------|\n",
        "| **Accuracy**     | \\((TP + TN) / (TP + TN + FP + FN)\\)          | Overall how often the model is correct             |\n",
        "| **Precision**    | \\(TP / (TP + FP)\\)                           | How many predicted positives are actually positive |\n",
        "| **Recall**       | \\(TP / (TP + FN)\\)                           | How many actual positives were correctly predicted |\n",
        "| **F1-Score**     | \\(2 \\times (Precision \\times Recall) / (Precision + Recall)\\) | Balance between precision and recall               |\n",
        "| **Specificity**  | \\(TN / (TN + FP)\\)                           | How well the model identifies actual negatives     |\n",
        "| **False Positive Rate (FPR)** | \\(FP / (FP + TN)\\)             | Rate of false alarms among actual negatives        |\n",
        "| **False Negative Rate (FNR)** | \\(FN / (FN + TP)\\)             | Miss rate among actual positives                   |\n",
        "| **Support**      | Number of actual samples per class            | Class distribution in your dataset                 |\n",
        "\n",
        "---\n",
        "\n",
        "###  **Example Calculation (Sample Numbers):**\n",
        "\n",
        "|                           | Predicted Positive | Predicted Negative |\n",
        "|---------------------------|--------------------|--------------------|\n",
        "| Actual Positive           | TP = 70            | FN = 30            |\n",
        "| Actual Negative           | FP = 20            | TN = 80            |\n",
        "\n",
        "Now calculate:\n",
        "\n",
        "- **Accuracy** = (70 + 80) / (70 + 30 + 20 + 80) = 150 / 200 = **0.75**\n",
        "- **Precision** = 70 / (70 + 20) = **0.78**\n",
        "- **Recall** = 70 / (70 + 30) = **0.70**\n",
        "- **F1-score** = 2 √ó (0.78 √ó 0.70) / (0.78 + 0.70) ‚âà **0.74**\n",
        "- **Specificity** = 80 / (80 + 20) = **0.80**\n",
        "\n",
        "---\n",
        "\n",
        "###  **Use Case Tips:**\n",
        "\n",
        "- Use **Precision** when **false positives** are costly (e.g., spam filters).\n",
        "- Use **Recall** when **false negatives** are costly (e.g., medical diagnoses).\n",
        "- Use **F1-score** for **imbalanced datasets** to balance precision and recall.\n",
        "- Use **Specificity** when you're interested in correctly identifying negatives.\n"
      ],
      "metadata": {
        "id": "gqkhfhMi6sSJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?\n",
        "Ans: \\\n",
        "\n",
        "###  **Confusion Matrix Recap:**\n",
        "\n",
        "|                           | **Predicted Positive** | **Predicted Negative** |\n",
        "|---------------------------|------------------------|------------------------|\n",
        "| **Actual Positive**       | **TP (True Positive)** | **FN (False Negative)** |\n",
        "| **Actual Negative**       | **FP (False Positive)** | **TN (True Negative)** |\n",
        "\n",
        "---\n",
        "\n",
        "###  **Accuracy Formula:**\n",
        "\n",
        "$$[\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "]$$\n",
        "\n",
        "**In Words:**  \n",
        "Accuracy is the **proportion of correct predictions** (both positives and negatives) out of all predictions.\n",
        "\n",
        "---\n",
        "\n",
        "###  **How Confusion Matrix Affects Accuracy:**\n",
        "\n",
        "- **Higher TP and TN** ‚Üí **Higher Accuracy** (model is predicting more correctly)\n",
        "- **Higher FP or FN** ‚Üí **Lower Accuracy** (model is making more mistakes)\n",
        "\n",
        "---\n",
        "\n",
        "###  **Caution with Imbalanced Data:**\n",
        "\n",
        "In **imbalanced datasets**, accuracy can be **misleading**.\n",
        "\n",
        " Example:  \n",
        "- Suppose 95% of your data is class A and only 5% is class B.  \n",
        "- A model that always predicts class A will be **95% accurate**, but **useless** for detecting class B.\n",
        "\n",
        "That‚Äôs why in such cases, we also rely on:\n",
        "- **Precision**\n",
        "- **Recall**\n",
        "- **F1-score**\n",
        "- **Confusion Matrix analysis**\n",
        "\n",
        "---\n",
        "\n",
        "###  Summary:\n",
        "\n",
        "| **Metric**   | **Relation to Confusion Matrix**                              |\n",
        "|--------------|---------------------------------------------------------------|\n",
        "| Accuracy     | Measures total correct predictions: $( (TP + TN) / Total )$   |\n",
        "| Misleading?  | Yes ‚Äî when the classes are imbalanced                         |\n",
        "| Use with     | Precision, Recall, F1-score, especially when data is skewed   |\n"
      ],
      "metadata": {
        "id": "zrevnN7-6slR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?\n",
        "Ans: \\\n",
        "\n",
        "A **confusion matrix** can help uncover **biases** and **weaknesses** in your model by showing where it makes mistakes ‚Äî especially across different classes.\n",
        "\n",
        "---\n",
        "\n",
        "###  **How to Spot Bias or Limitations:**\n",
        "\n",
        "#### 1. **Class Imbalance Bias**\n",
        "- **What to look for:** Model predicts the **majority class** most of the time.\n",
        "- **In the matrix:** Very high **TN** or **TP** for one class, high **FN** or **FP** for the other.\n",
        "-  **Solution:** Use techniques like oversampling, undersampling, or class weighting.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Underperforming on Minority Class**\n",
        "- **What to look for:** High **FN** (missing real positives) or high **FP** (false alarms).\n",
        "- **Example:** In fraud detection, if most fraud cases are **FN**, the model isn't catching them.\n",
        "-  **Solution:** Improve recall or precision, or use a different threshold.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Overfitting or Underfitting**\n",
        "- **What to look for:** High accuracy but still many **FP/FN**.\n",
        "- Could mean the model is memorizing or too simplistic.\n",
        "-  **Solution:** Try more complex models or tune hyperparameters.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Unequal Treatment of Classes (Bias)**\n",
        "- **What to look for:** Model favors one class, performs poorly on the other(s).\n",
        "- **In the matrix:** One class has high TP/TN, the other has high FN/FP.\n",
        "-  **Solution:** Ensure fair representation of all classes in training data.\n",
        "\n",
        "---\n",
        "\n",
        "###  Example:\n",
        "\n",
        "|                           | Predicted Positive | Predicted Negative |\n",
        "|---------------------------|--------------------|--------------------|\n",
        "| **Actual Positive**       | **TP = 10**        | **FN = 90**        |\n",
        "| **Actual Negative**       | **FP = 5**         | **TN = 95**        |\n",
        "\n",
        "- **Model seems accurate** (TP+TN = 105/200 = 52.5%)\n",
        "- **But... it's missing most real positives!** (Recall = 10 / (10+90) = **0.10** )\n",
        "\n",
        " **This shows a serious bias toward predicting negatives**, likely due to class imbalance.\n"
      ],
      "metadata": {
        "id": "2k2kt7Kv6s2w"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gO8miRmwG5VD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}