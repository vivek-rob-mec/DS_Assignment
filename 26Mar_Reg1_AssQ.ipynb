{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an example of each.\n",
        "Ans: \\\n",
        "###  **Simple Linear Regression**\n",
        "- **Definition**: A method to model the relationship between **one independent variable (X)** and **one dependent variable (Y)** using a straight line.\n",
        "- **Equation**:  \n",
        "  $$\n",
        "  [\n",
        "  Y = \\beta_0 + \\beta_1 X + \\varepsilon\n",
        "  ]\n",
        "  $$\n",
        "  Where:\n",
        "  - $( Y )$ = dependent variable (output)\n",
        "  - $( X )$ = independent variable (input)\n",
        "  - $( \\beta_0 )$ = intercept\n",
        "  - $( \\beta_1 )$ = slope\n",
        "  - $( \\varepsilon $) = error term\n",
        "\n",
        "- **Example**:  \n",
        "  Predicting a student’s exam score based on hours studied.\n",
        "  ```python\n",
        "  Y = Exam_Score  \n",
        "  X = Hours_Studied\n",
        "  ```\n",
        "\n",
        "---\n",
        "\n",
        "###  **Multiple Linear Regression**\n",
        "- **Definition**: A method to model the relationship between **two or more independent variables (X1, X2, ..., Xn)** and a single dependent variable (Y).\n",
        "- **Equation**:  \n",
        "  $$[\n",
        "  Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\varepsilon\n",
        "  ]\n",
        "  $$\n",
        "\n",
        "- **Example**:  \n",
        "  Predicting a student’s exam score based on hours studied, number of practice tests taken, and sleep hours.\n",
        "  ```python\n",
        "  Y = Exam_Score  \n",
        "  X1 = Hours_Studied  \n",
        "  X2 = Practice_Tests  \n",
        "  X3 = Sleep_Hours\n",
        "  ```"
      ],
      "metadata": {
        "id": "sd4PRhPEn1iA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in a given dataset?\n",
        "Ans: \\\n",
        "\n",
        "Linear regression is based on several key assumptions. Ensuring these assumptions hold is crucial for building a reliable and interpretable model.\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Linearity**\n",
        "- **Assumption**: There is a linear relationship between the independent variables and the dependent variable.\n",
        "- **How to Check**:\n",
        "  - Plot the actual vs. predicted values.\n",
        "  - Use a scatter plot of residuals vs. predicted values—there should be no clear pattern.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Independence of Errors**\n",
        "- **Assumption**: The residuals (errors) are independent of each other.\n",
        "- **How to Check**:\n",
        "  - Use the Durbin-Watson test (mainly for time series data).\n",
        "  - Plot residuals in the order of observation and look for patterns or autocorrelation.\n",
        "\n",
        "---\n",
        "\n",
        "### 3. **Homoscedasticity (Constant Variance of Errors)**\n",
        "- **Assumption**: The variance of residuals is constant across all levels of the independent variables.\n",
        "- **How to Check**:\n",
        "  - Plot residuals vs. predicted values. The spread should be roughly constant (not a funnel shape).\n",
        "  - Perform the Breusch-Pagan test.\n",
        "\n",
        "---\n",
        "\n",
        "### 4. **Normality of Residuals**\n",
        "- **Assumption**: The residuals are normally distributed.\n",
        "- **How to Check**:\n",
        "  - Use a histogram or Q-Q plot of residuals.\n",
        "  - Apply statistical tests like the Shapiro-Wilk test or Kolmogorov-Smirnov test.\n",
        "\n",
        "---\n",
        "\n",
        "### 5. **No Multicollinearity (for multiple linear regression)**\n",
        "- **Assumption**: Independent variables are not highly correlated with each other.\n",
        "- **How to Check**:\n",
        "  - Calculate the Variance Inflation Factor (VIF) for each predictor. A VIF above 5 (or 10) indicates multicollinearity.\n",
        "  - Check the correlation matrix."
      ],
      "metadata": {
        "id": "ujMseS9KpBn7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using a real-world scenario.\n",
        "Ans: \\\n",
        "In a linear regression model, the equation is typically written as:\n",
        "\n",
        "$$[\n",
        "Y = \\beta_0 + \\beta_1 X + \\varepsilon\n",
        "]\n",
        "$$\n",
        "Where:\n",
        "\n",
        "- \\( Y \\): Dependent variable (output)\n",
        "- \\( X \\): Independent variable (input)\n",
        "- $( \\beta_0 )$: Intercept\n",
        "- $( \\beta_1 )$: Slope (coefficient of $( X )$)\n",
        "- $( \\varepsilon )$: Error term\n",
        "\n",
        "---\n",
        "\n",
        "### 1. **Intercept $(( \\beta_0 ))$**\n",
        "- This is the predicted value of \\( Y \\) when \\( X = 0 \\).\n",
        "- In many real-world contexts, the intercept may not have a meaningful interpretation if \\( X = 0 \\) is unrealistic.\n",
        "\n",
        "---\n",
        "\n",
        "### 2. **Slope $(( \\beta_1 ))$**\n",
        "- This indicates the **change in \\( Y \\)** for a **one-unit increase in \\( X \\)**.\n",
        "- It shows the direction and strength of the relationship between \\( X \\) and \\( Y \\).\n",
        "\n",
        "---\n",
        "\n",
        "###  Example: Predicting Salary Based on Years of Experience\n",
        "\n",
        "Suppose you build a linear regression model:\n",
        "\n",
        "$$[\n",
        "\\text{Salary} = 30{,}000 + 8{,}000 \\times (\\text{Years of Experience})\n",
        "]\n",
        "$$\n",
        "#### Interpretation:\n",
        "- **Intercept (30,000)**: This is the estimated starting salary for someone with 0 years of experience. It’s the base salary.\n",
        "- **Slope (8,000)**: For each additional year of experience, the salary is expected to increase by $8,000.\n",
        "\n",
        "---\n",
        "\n",
        "###  Real-World Meaning:\n",
        "If a person has 5 years of experience:\n",
        "$$\n",
        "[\n",
        "\\text{Salary} = 30{,}000 + 8{,}000 \\times 5 = 70{,}000\n",
        "]\n",
        "$$\n",
        "This model tells us that **experience has a positive and consistent impact** on salary, as represented by the positive slope."
      ],
      "metadata": {
        "id": "XmAZurNxqGyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q4. Explain the concept of gradient descent. How is it used in machine learning?\n",
        "Ans: \\\n",
        "\n",
        "**Gradient Descent** is an optimization algorithm commonly used in machine learning to minimize the cost (or loss) function of a model. The goal is to find the set of parameters (weights and biases) that result in the best performance by reducing the error between predicted and actual values.\n",
        "\n",
        "---\n",
        "\n",
        "#### What Is Gradient Descent?\n",
        "\n",
        "Gradient descent works by iteratively adjusting the model's parameters in the direction that reduces the cost function. It uses the gradient (or slope) of the cost function to determine the direction and magnitude of the change.\n",
        "\n",
        "Mathematically, the update rule is:\n",
        "\n",
        "$$[\n",
        "\\theta = \\theta - \\alpha \\cdot \\nabla J(\\theta)\n",
        "]\n",
        "$$\n",
        "Where:\n",
        "\n",
        "- \\( \\theta \\) represents the model's parameters\n",
        "- \\( \\alpha \\) is the learning rate, a small value that controls the step size\n",
        "- \\( \\nabla J(\\theta) \\) is the gradient of the cost function with respect to the parameters\n",
        "\n",
        "---\n",
        "\n",
        "#### Why Is It Used in Machine Learning?\n",
        "\n",
        "In machine learning, models are trained by minimizing a cost function that measures how far the model's predictions are from the actual target values. Gradient descent helps find the values of parameters that minimize this cost, making the model more accurate.\n",
        "\n",
        "---\n",
        "\n",
        "#### How Gradient Descent Works\n",
        "\n",
        "1. Start with initial guesses for the parameters (usually random)\n",
        "2. Calculate the gradient of the cost function\n",
        "3. Update the parameters using the gradient and learning rate\n",
        "4. Repeat the process until the model converges (i.e., the cost function stops decreasing)\n",
        "\n",
        "---\n",
        "\n",
        "#### Types of Gradient Descent\n",
        "\n",
        "- **Batch Gradient Descent**: Uses the entire dataset to compute gradients for each step\n",
        "- **Stochastic Gradient Descent (SGD)**: Uses one training example per step; faster but noisier\n",
        "- **Mini-Batch Gradient Descent**: Uses a small batch of training examples; balances speed and stability\n",
        "\n",
        "---\n",
        "\n",
        "#### Example in Linear Regression\n",
        "\n",
        "In linear regression, the cost function is typically the mean squared error:\n",
        "$$\n",
        "[\n",
        "J(\\theta) = \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2\n",
        "]\n",
        "$$\n",
        "Gradient descent is used to adjust the weights so that the predicted values $( \\hat{y}_i )$ get closer to the actual values $( y_i )$, minimizing this error."
      ],
      "metadata": {
        "id": "_4VFsghartKb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?\n",
        "Ans: \\\n",
        "\n",
        "Multiple Linear Regression is a statistical method used to model the relationship between **one dependent variable** and **two or more independent variables**. It is an extension of simple linear regression, which involves only one independent variable.\n",
        "\n",
        "The general form of the multiple linear regression model is:\n",
        "$$\n",
        "[\n",
        "Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\varepsilon\n",
        "]\n",
        "$$\n",
        "Where:\n",
        "- $( Y )$ is the dependent variable (what you are trying to predict)\n",
        "- $( X_1, X_2, \\dots, X_n )$ are the independent variables (predictors)\n",
        "- $( \\beta_0 )$ is the intercept\n",
        "- $( \\beta_1, \\beta_2, \\dots, \\beta_n )$ are the coefficients of the independent variables\n",
        "- $( \\varepsilon )$ is the error term\n",
        "\n",
        "---\n",
        "\n",
        "#### Example\n",
        "\n",
        "Suppose you're predicting house prices based on several features:\n",
        "- $( X_1 )$: Size of the house (in square feet)\n",
        "- $( X_2 )$: Number of bedrooms\n",
        "- $( X_3 )$: Distance to the city center (in km)\n",
        "\n",
        "The model might look like this:\n",
        "\n",
        "$$[\n",
        "\\text{Price} = \\beta_0 + \\beta_1 (\\text{Size}) + \\beta_2 (\\text{Bedrooms}) + \\beta_3 (\\text{Distance}) + \\varepsilon\n",
        "]$$\n",
        "\n",
        "Each coefficient represents the effect of one predictor on the house price, assuming the others are held constant.\n",
        "\n",
        "---\n",
        "\n",
        "#### How It Differs from Simple Linear Regression\n",
        "\n",
        "| Feature                        | Simple Linear Regression                     | Multiple Linear Regression                        |\n",
        "|-------------------------------|----------------------------------------------|--------------------------------------------------|\n",
        "| Number of independent variables | One                                           | Two or more                                      |\n",
        "| Model equation                 | $( Y = \\beta_0 + \\beta_1 X + \\varepsilon )$   | $( Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_n X_n + \\varepsilon )$ |\n",
        "| Complexity                     | Low                                           | Higher (may require more advanced techniques)    |\n",
        "| Use case example               | Predicting salary from years of experience    | Predicting salary from experience, education, and location |\n",
        "\n",
        "---\n",
        "\n",
        "#### When to Use Multiple Linear Regression\n",
        "\n",
        "Use multiple linear regression when:\n",
        "- Your target variable is continuous\n",
        "- You believe multiple factors influence the outcome\n",
        "- You want to quantify the individual effect of each variable while controlling for others\n"
      ],
      "metadata": {
        "id": "RVBvREBTs34c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and address this issue?\n",
        "Ans: \\\n",
        "\n",
        "**Multicollinearity** occurs in a multiple linear regression model when **two or more independent variables are highly correlated** with each other. This means that one predictor variable can be linearly predicted from the others with a high degree of accuracy.\n",
        "\n",
        "When multicollinearity is present:\n",
        "- It becomes difficult to determine the individual effect of each predictor on the dependent variable.\n",
        "- Coefficient estimates may become **unstable**, **highly sensitive to small changes in data**, and **statistically insignificant**, even if they are actually important.\n",
        "\n",
        "---\n",
        "\n",
        "#### Why is Multicollinearity a Problem?\n",
        "\n",
        "- It **inflates the standard errors** of the coefficients.\n",
        "- It reduces the **reliability of statistical tests** for the coefficients (like t-tests).\n",
        "- It makes it hard to **interpret the model**, as changes in one variable may mirror changes in another.\n",
        "\n",
        "---\n",
        "\n",
        "#### How to Detect Multicollinearity\n",
        "\n",
        "1. **Correlation Matrix**:\n",
        "   - Compute the pairwise correlation between independent variables.\n",
        "   - Correlation values close to +1 or -1 indicate potential multicollinearity.\n",
        "\n",
        "2. **Variance Inflation Factor (VIF)**:\n",
        "   - VIF measures how much the variance of a regression coefficient is inflated due to multicollinearity.\n",
        "   - A VIF above **5 or 10** is typically a sign of multicollinearity.\n",
        "\n",
        "   Example in Python:\n",
        "   ```python\n",
        "   from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "   import pandas as pd\n",
        "\n",
        "   # Assume X is your DataFrame of independent variables\n",
        "   vif_data = pd.DataFrame()\n",
        "   vif_data[\"Feature\"] = X.columns\n",
        "   vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
        "   print(vif_data)\n",
        "   ```\n",
        "\n",
        "3. **Condition Number**:\n",
        "   - A condition number above 30 may indicate multicollinearity.\n",
        "\n",
        "---\n",
        "\n",
        "#### How to Address Multicollinearity\n",
        "\n",
        "1. **Remove one or more correlated predictors**:\n",
        "   - If two features are highly correlated, consider dropping one of them.\n",
        "\n",
        "2. **Combine correlated variables**:\n",
        "   - Create a new feature by combining correlated variables (e.g., using their average or principal component).\n",
        "\n",
        "3. **Principal Component Analysis (PCA)**:\n",
        "   - PCA reduces the dimensionality of data and removes correlations between predictors.\n",
        "\n",
        "4. **Regularization Techniques**:\n",
        "   - Use regression techniques like **Ridge** or **Lasso**, which can handle multicollinearity by penalizing large coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "#### Summary\n",
        "\n",
        "| Detection Method           | Fix/Resolution                            |\n",
        "|---------------------------|--------------------------------------------|\n",
        "| Correlation matrix         | Drop or combine highly correlated features |\n",
        "| VIF > 5 or 10              | Remove or transform variables              |\n",
        "| Condition number > 30      | Use PCA or regularization techniques       |"
      ],
      "metadata": {
        "id": "ZV8eolzDuo2B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q7. Describe the polynomial regression model. How is it different from linear regression?\n",
        "Ans: \\\n",
        "\n",
        "**Polynomial regression** is a type of regression analysis in which the relationship between the independent variable \\( X \\) and the dependent variable \\( Y \\) is modeled as an \\( n \\)th-degree polynomial.\n",
        "\n",
        "It is used when the data shows a **non-linear** relationship but can still be fit using a **linear model structure** by transforming the features.\n",
        "\n",
        "The general form of a polynomial regression model is:\n",
        "\n",
        "$$[\n",
        "Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3 + \\dots + \\beta_n X^n + \\varepsilon\n",
        "]$$\n",
        "\n",
        "Where:\n",
        "- $( X^2, X^3, ..., X^n )$ are the higher-degree polynomial terms\n",
        "- $( \\beta_0, \\beta_1, ..., \\beta_n )$ are the coefficients to be estimated\n",
        "- $( \\varepsilon )$ is the error term\n",
        "\n",
        "---\n",
        "\n",
        "#### Example\n",
        "\n",
        "Suppose you're modeling the relationship between advertising spend and sales. A simple linear model may not fit well because the increase in sales may slow down after a certain point.\n",
        "\n",
        "Using polynomial regression of degree 2:\n",
        "\n",
        "$$[\n",
        "\\text{Sales} = \\beta_0 + \\beta_1 (\\text{Spend}) + \\beta_2 (\\text{Spend})^2 + \\varepsilon\n",
        "]$$\n",
        "\n",
        "This allows the model to capture the curvature in the data.\n",
        "\n",
        "---\n",
        "\n",
        "#### Difference Between Polynomial and Linear Regression\n",
        "\n",
        "| Feature                      | Linear Regression                              | Polynomial Regression                          |\n",
        "|-----------------------------|-------------------------------------------------|------------------------------------------------|\n",
        "| Model Equation               | $( Y = \\beta_0 + \\beta_1 X + \\varepsilon )$     | $( Y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\dots + \\beta_n X^n + \\varepsilon )$ |\n",
        "| Relationship Modeled        | Straight line (linear)                          | Curved (non-linear)                            |\n",
        "| Complexity                  | Simpler                                         | More complex depending on the degree of the polynomial |\n",
        "| Use Case Example            | Predicting salary from years of experience      | Predicting crop yield from temperature over time |\n",
        "| Overfitting Risk            | Lower                                           | Higher (especially with higher-degree polynomials) |\n",
        "\n",
        "---\n",
        "\n",
        "#### Key Notes\n",
        "\n",
        "- Although the equation includes powers of \\( X \\), **polynomial regression is still a linear model** in terms of the coefficients.\n",
        "- Polynomial regression is suitable when the trend in the data is **non-linear but continuous and smooth**.\n",
        "- Choosing the **right degree** of the polynomial is important. Too low may underfit, too high may overfit."
      ],
      "metadata": {
        "id": "ff_hagbqwMcp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q8. What are the advantages and disadvantages of polynomial regression compared to linear regression? In what situations would you prefer to use polynomial regression?\n",
        "Ans: \\\n",
        "\n",
        "#### Advantages of Polynomial Regression\n",
        "\n",
        "1. **Captures Non-Linear Relationships**  \n",
        "   Polynomial regression can model more complex, curved relationships between the input and output variables that linear regression cannot.\n",
        "\n",
        "2. **Flexible Model**  \n",
        "   By increasing the degree of the polynomial, the model becomes more flexible and can better fit datasets with complex patterns.\n",
        "\n",
        "3. **Still Linear in Parameters**  \n",
        "   Although it models non-linear relationships, it is still linear in terms of the coefficients, making it easier to estimate using standard linear regression techniques.\n",
        "\n",
        "---\n",
        "\n",
        "#### Disadvantages of Polynomial Regression\n",
        "\n",
        "1. **Risk of Overfitting**  \n",
        "   Higher-degree polynomials can fit the training data too closely, capturing noise rather than the true pattern, which reduces the model’s ability to generalize.\n",
        "\n",
        "2. **Poor Extrapolation**  \n",
        "   Polynomial models can behave unpredictably outside the range of the training data, especially with high degrees.\n",
        "\n",
        "3. **Computational Complexity**  \n",
        "   As the degree increases, the model becomes more complex and computationally expensive.\n",
        "\n",
        "4. **Interpretability**  \n",
        "   Unlike linear regression, polynomial models are harder to interpret because the effect of each predictor is not constant across the input range.\n",
        "\n",
        "---\n",
        "\n",
        "#### When to Prefer Polynomial Regression\n",
        "\n",
        "Use polynomial regression when:\n",
        "- The relationship between the independent and dependent variable is **non-linear** and cannot be captured by a straight line.\n",
        "- A plot of the data suggests **curvature or trends** that a linear model fails to represent.\n",
        "- The dataset is not too large and the degree of the polynomial is moderate (to avoid overfitting).\n",
        "\n",
        "---\n",
        "\n",
        "#### Example Scenario\n",
        "\n",
        "Suppose you're analyzing the effect of **temperature on crop yield**. A linear model might suggest yield increases steadily with temperature, but real-world data may show that yield increases up to a point and then decreases. Polynomial regression would be better suited to capture this kind of curved trend."
      ],
      "metadata": {
        "id": "Vd6Xt95SxNtA"
      }
    }
  ]
}